{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in d:\\softwares\\anaconda\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pybind11>=2.2 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (2.6.0)\n",
      "Requirement already satisfied: numpy in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (1.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()\n",
    "fasttext.util.reduce_model(ft, 200)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens, tags = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "#processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_Hindi_tokens = pd.read_csv(\"hindi_tokens_translated_to_English_list.csv\", sep=\"\\s+\")\n",
    "#processed_Hindi_tokens = translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    idxs = to_ix[seq]\n",
    "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "    idxs = idxs.view(1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_padded_sentence(sentence,word_to_ix):\n",
    "    \n",
    "    # map sentences to vocab\n",
    "    sentence =  [[word_to_ix[word] for word in sent] for sent in sentence]\n",
    "    # sentence now looks like:  \n",
    "    # [[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]\n",
    "    sentence_lengths = [len(sent) for sent in sentence]\n",
    "    pad_token = word_to_ix['<PAD>']\n",
    "    longest_sent = max(sentence_lengths)\n",
    "    batch_size = len(sentence)\n",
    "    padded_sentence = np.ones((batch_size, longest_sent)) * pad_token\n",
    "    for i, x_len in enumerate(sentence_lengths):\n",
    "        sequence = sentence[i]\n",
    "        padded_sentence[i, 0:x_len] = sequence[:x_len]\n",
    "  \n",
    "    return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "word_to_ix = {\"<PAD>\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11999\n"
     ]
    }
   ],
   "source": [
    "#training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "training_data = utils.substitute_with_UNK(processed_tokens,word_to_ix)\n",
    "testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = ft.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " '</s>',\n",
       " 'in',\n",
       " 'is',\n",
       " ':',\n",
       " 'I',\n",
       " 'for',\n",
       " 'that',\n",
       " ')',\n",
       " '\"',\n",
       " '(',\n",
       " 'on',\n",
       " 'with',\n",
       " 'it',\n",
       " 'you',\n",
       " 'The',\n",
       " 'was',\n",
       " 'as',\n",
       " 'are',\n",
       " 'at',\n",
       " '/',\n",
       " '’',\n",
       " 'be',\n",
       " 'by',\n",
       " \"'s\",\n",
       " 'this',\n",
       " 'have',\n",
       " 'from',\n",
       " 'or',\n",
       " '!',\n",
       " 'not',\n",
       " 'your',\n",
       " 'an',\n",
       " \"'\",\n",
       " 'but',\n",
       " '?',\n",
       " 'can',\n",
       " '-',\n",
       " 'will',\n",
       " 's',\n",
       " 'my',\n",
       " 'has',\n",
       " 'all',\n",
       " 'we',\n",
       " 'they',\n",
       " 'he',\n",
       " 'his',\n",
       " 'more',\n",
       " 'one',\n",
       " 'about',\n",
       " 'their',\n",
       " \"'t\",\n",
       " 'so',\n",
       " 'which',\n",
       " 'It',\n",
       " 'out',\n",
       " 'up',\n",
       " '...',\n",
       " 'were',\n",
       " 'had',\n",
       " 'who',\n",
       " 'like',\n",
       " ';',\n",
       " '“',\n",
       " 'our',\n",
       " 'would',\n",
       " '”',\n",
       " 'time',\n",
       " 'been',\n",
       " 'if',\n",
       " 'also',\n",
       " 'just',\n",
       " 'when',\n",
       " 'her',\n",
       " 'This',\n",
       " 'me',\n",
       " 'there',\n",
       " 'do',\n",
       " 'what',\n",
       " 'some',\n",
       " 'other',\n",
       " 'In',\n",
       " 'them',\n",
       " '–',\n",
       " '1',\n",
       " 'get',\n",
       " 'new',\n",
       " 'into',\n",
       " '&',\n",
       " 'We',\n",
       " 'than',\n",
       " 'A',\n",
       " 'no',\n",
       " 'only',\n",
       " 'first',\n",
       " 'any',\n",
       " 'its',\n",
       " 'people',\n",
       " '2',\n",
       " '$',\n",
       " 'very',\n",
       " 't',\n",
       " 'over',\n",
       " 'she',\n",
       " '%',\n",
       " 'how',\n",
       " 'make',\n",
       " 'You',\n",
       " 'said',\n",
       " 'He',\n",
       " 'two',\n",
       " 'may',\n",
       " 'know',\n",
       " 'then',\n",
       " 'see',\n",
       " 'after',\n",
       " 'most',\n",
       " 'good',\n",
       " 'years',\n",
       " 'If',\n",
       " 'these',\n",
       " 'now',\n",
       " '3',\n",
       " 'use',\n",
       " 'because',\n",
       " 'well',\n",
       " 'work',\n",
       " 'could',\n",
       " 'us',\n",
       " 'don',\n",
       " 'way',\n",
       " 'much',\n",
       " 'back',\n",
       " 'many',\n",
       " 'think',\n",
       " 'where',\n",
       " 'even',\n",
       " 'him',\n",
       " 'through',\n",
       " 'am',\n",
       " '10',\n",
       " '|',\n",
       " 'here',\n",
       " '#',\n",
       " 'made',\n",
       " 'year',\n",
       " 'should',\n",
       " '*',\n",
       " 'really',\n",
       " 'being',\n",
       " 'such',\n",
       " 'need',\n",
       " 'great',\n",
       " 'And',\n",
       " ']',\n",
       " '4',\n",
       " '[',\n",
       " '5',\n",
       " 'day',\n",
       " 'before',\n",
       " 'want',\n",
       " 'used',\n",
       " 'go',\n",
       " 'those',\n",
       " '…',\n",
       " 'But',\n",
       " 'right',\n",
       " \"'m\",\n",
       " 'take',\n",
       " '—',\n",
       " 'May',\n",
       " 'still',\n",
       " 'last',\n",
       " 'off',\n",
       " 'too',\n",
       " 'New',\n",
       " 'going',\n",
       " 'best',\n",
       " 'find',\n",
       " 'love',\n",
       " 'did',\n",
       " 'while',\n",
       " 'home',\n",
       " 'There',\n",
       " 'They',\n",
       " 'same',\n",
       " 'around',\n",
       " 'help',\n",
       " 'down',\n",
       " 'information',\n",
       " 'UTC',\n",
       " 'place',\n",
       " 'i',\n",
       " '2017',\n",
       " 'For',\n",
       " 'little',\n",
       " 'life',\n",
       " 'between',\n",
       " 'each',\n",
       " 'own',\n",
       " 'both',\n",
       " '12',\n",
       " '6',\n",
       " 'world',\n",
       " 'part',\n",
       " 'few',\n",
       " '8',\n",
       " '7',\n",
       " 'talk',\n",
       " 'As',\n",
       " 'look',\n",
       " '2012',\n",
       " 'things',\n",
       " '11',\n",
       " 'say',\n",
       " 'does',\n",
       " 'every',\n",
       " 'something',\n",
       " '2013',\n",
       " 'during',\n",
       " 'got',\n",
       " 'So',\n",
       " \"'ve\",\n",
       " 'What',\n",
       " 'since',\n",
       " 'found',\n",
       " 'long',\n",
       " 'different',\n",
       " 'says',\n",
       " '>',\n",
       " 'never',\n",
       " 'another',\n",
       " '�',\n",
       " 'Ã',\n",
       " 'better',\n",
       " '2016',\n",
       " 'using',\n",
       " '+',\n",
       " 'free',\n",
       " '20',\n",
       " 'under',\n",
       " 'three',\n",
       " 'family',\n",
       " 'She',\n",
       " 'including',\n",
       " 'That',\n",
       " 'always',\n",
       " '\\\\',\n",
       " 'next',\n",
       " 'come',\n",
       " 'without',\n",
       " 'My',\n",
       " 'again',\n",
       " '9',\n",
       " 'game',\n",
       " \"'re\",\n",
       " '15',\n",
       " '2011',\n",
       " 'When',\n",
       " 'days',\n",
       " 'set',\n",
       " '30',\n",
       " 'All',\n",
       " 'number',\n",
       " '2014',\n",
       " 'end',\n",
       " 'lot',\n",
       " 'business',\n",
       " 'sure',\n",
       " 'system',\n",
       " 'book',\n",
       " '2015',\n",
       " 'against',\n",
       " 'high',\n",
       " '=',\n",
       " '2010',\n",
       " 'must',\n",
       " 'available',\n",
       " 'To',\n",
       " 'might',\n",
       " 'show',\n",
       " 'area',\n",
       " 'Â',\n",
       " 'week',\n",
       " '00',\n",
       " 'away',\n",
       " 'team',\n",
       " 'March',\n",
       " 'name',\n",
       " 'until',\n",
       " 'April',\n",
       " 'give',\n",
       " 'thing',\n",
       " 'read',\n",
       " 'put',\n",
       " \"'ll\",\n",
       " 'On',\n",
       " 'small',\n",
       " 'school',\n",
       " 'feel',\n",
       " 'second',\n",
       " 'company',\n",
       " 'June',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'page',\n",
       " 'keep',\n",
       " 'top',\n",
       " 'why',\n",
       " 'January',\n",
       " 'site',\n",
       " '18',\n",
       " 'post',\n",
       " 'today',\n",
       " '16',\n",
       " 'within',\n",
       " '0',\n",
       " 'service',\n",
       " 'having',\n",
       " 'looking',\n",
       " 'American',\n",
       " 'point',\n",
       " 'data',\n",
       " 'though',\n",
       " '½',\n",
       " 'July',\n",
       " 'University',\n",
       " 'full',\n",
       " '14',\n",
       " 'm',\n",
       " 'able',\n",
       " 'left',\n",
       " 'With',\n",
       " 'support',\n",
       " 'United',\n",
       " '2009',\n",
       " '13',\n",
       " 'experience',\n",
       " 'room',\n",
       " '¿',\n",
       " 'water',\n",
       " 'state',\n",
       " '‘',\n",
       " 'big',\n",
       " 'order',\n",
       " 'October',\n",
       " 'No',\n",
       " 'article',\n",
       " 'case',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'ago',\n",
       " 'house',\n",
       " 'September',\n",
       " '17',\n",
       " 'children',\n",
       " 'One',\n",
       " 'local',\n",
       " 'December',\n",
       " 'start',\n",
       " 'times',\n",
       " 'February',\n",
       " 'called',\n",
       " 'August',\n",
       " '25',\n",
       " 'November',\n",
       " 'ever',\n",
       " 'More',\n",
       " '2008',\n",
       " 'done',\n",
       " 'change',\n",
       " 'play',\n",
       " 'group',\n",
       " 'working',\n",
       " 'based',\n",
       " 'online',\n",
       " 'http',\n",
       " 'course',\n",
       " 'least',\n",
       " 'doesn',\n",
       " 'money',\n",
       " 'won',\n",
       " 'man',\n",
       " 'less',\n",
       " '@',\n",
       " 'important',\n",
       " 'After',\n",
       " '24',\n",
       " 'How',\n",
       " '22',\n",
       " 'try',\n",
       " 'getting',\n",
       " 'thought',\n",
       " 'public',\n",
       " 'actually',\n",
       " 'already',\n",
       " 'night',\n",
       " 'person',\n",
       " 're',\n",
       " '21',\n",
       " 'went',\n",
       " '..',\n",
       " 'story',\n",
       " 'several',\n",
       " '--',\n",
       " 'later',\n",
       " 'makes',\n",
       " 'side',\n",
       " 'list',\n",
       " 'following',\n",
       " '19',\n",
       " 'came',\n",
       " 'By',\n",
       " 'doing',\n",
       " 'power',\n",
       " 'large',\n",
       " 'season',\n",
       " 'provide',\n",
       " 'website',\n",
       " 'bit',\n",
       " 'far',\n",
       " 'real',\n",
       " 'known',\n",
       " 'took',\n",
       " 'PM',\n",
       " 'city',\n",
       " '23',\n",
       " 'let',\n",
       " 'At',\n",
       " 'along',\n",
       " 'together',\n",
       " 'per',\n",
       " '}',\n",
       " 'often',\n",
       " 'These',\n",
       " 'music',\n",
       " 'hard',\n",
       " 'God',\n",
       " 'share',\n",
       " 'Your',\n",
       " 'Our',\n",
       " 'line',\n",
       " 'process',\n",
       " 'care',\n",
       " 'others',\n",
       " 'open',\n",
       " 'John',\n",
       " 'car',\n",
       " 'services',\n",
       " 'include',\n",
       " 'food',\n",
       " 'started',\n",
       " 'easy',\n",
       " 'country',\n",
       " 'months',\n",
       " 'fact',\n",
       " 'women',\n",
       " 'yet',\n",
       " 'students',\n",
       " '2007',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'live',\n",
       " 'problem',\n",
       " 'run',\n",
       " 'video',\n",
       " 've',\n",
       " 'become',\n",
       " 'government',\n",
       " 'everything',\n",
       " 'series',\n",
       " 'However',\n",
       " 'anything',\n",
       " 'four',\n",
       " 'pm',\n",
       " 'means',\n",
       " 'stay',\n",
       " '100',\n",
       " 'early',\n",
       " 'possible',\n",
       " 'past',\n",
       " 'design',\n",
       " 'quality',\n",
       " 'City',\n",
       " 'seen',\n",
       " 'States',\n",
       " 'given',\n",
       " 'pretty',\n",
       " 'State',\n",
       " 'blog',\n",
       " 'U.S.',\n",
       " \"'d\",\n",
       " 'film',\n",
       " 'nice',\n",
       " 'history',\n",
       " 'job',\n",
       " 'call',\n",
       " 'Now',\n",
       " 'kind',\n",
       " 'believe',\n",
       " 'community',\n",
       " 'needs',\n",
       " 'level',\n",
       " 'program',\n",
       " 'body',\n",
       " 'view',\n",
       " 'friends',\n",
       " 'control',\n",
       " 'comes',\n",
       " 'York',\n",
       " 'products',\n",
       " 'form',\n",
       " 'above',\n",
       " '26',\n",
       " 'School',\n",
       " 'health',\n",
       " 'minutes',\n",
       " 'probably',\n",
       " 'World',\n",
       " 'hours',\n",
       " 'fun',\n",
       " 'National',\n",
       " 'US',\n",
       " 'either',\n",
       " 'County',\n",
       " 'head',\n",
       " '28',\n",
       " 'example',\n",
       " 'product',\n",
       " 'please',\n",
       " 'close',\n",
       " 'beautiful',\n",
       " 'market',\n",
       " 'across',\n",
       " 'whole',\n",
       " 'offer',\n",
       " 'quite',\n",
       " 'trying',\n",
       " 'price',\n",
       " 'told',\n",
       " 'idea',\n",
       " 'members',\n",
       " 'light',\n",
       " '27',\n",
       " 'million',\n",
       " 'single',\n",
       " 'His',\n",
       " 'perfect',\n",
       " 'Please',\n",
       " 'hope',\n",
       " 'access',\n",
       " 'research',\n",
       " 'due',\n",
       " '2006',\n",
       " 'tell',\n",
       " 'seems',\n",
       " 'South',\n",
       " 'added',\n",
       " 'almost',\n",
       " 'current',\n",
       " 'short',\n",
       " 'bad',\n",
       " 'Not',\n",
       " '50',\n",
       " 'games',\n",
       " 'project',\n",
       " 'review',\n",
       " 'hand',\n",
       " 'email',\n",
       " 'near',\n",
       " 'below',\n",
       " 'men',\n",
       " 'wanted',\n",
       " '29',\n",
       " 'create',\n",
       " 'nothing',\n",
       " 'question',\n",
       " 'de',\n",
       " 'special',\n",
       " 'everyone',\n",
       " 'content',\n",
       " 'Posted',\n",
       " 'future',\n",
       " 'rather',\n",
       " 'check',\n",
       " 'taking',\n",
       " 'Thanks',\n",
       " 'works',\n",
       " 'From',\n",
       " 'month',\n",
       " 'front',\n",
       " 'law',\n",
       " 'space',\n",
       " 'news',\n",
       " 'Here',\n",
       " 'reason',\n",
       " 'add',\n",
       " 'isn',\n",
       " 'anyone',\n",
       " 'report',\n",
       " 'whether',\n",
       " 'major',\n",
       " 'main',\n",
       " 'young',\n",
       " 'development',\n",
       " 'North',\n",
       " 'event',\n",
       " '•',\n",
       " 'results',\n",
       " 'personal',\n",
       " 'especially',\n",
       " 'mean',\n",
       " 'issue',\n",
       " 'five',\n",
       " 'social',\n",
       " 'original',\n",
       " 'age',\n",
       " '....',\n",
       " 'born',\n",
       " 'taken',\n",
       " 'living',\n",
       " 'buy',\n",
       " 'd',\n",
       " 'll',\n",
       " 'Reply',\n",
       " 'reading',\n",
       " 'English',\n",
       " 'mind',\n",
       " 'study',\n",
       " 'building',\n",
       " 'result',\n",
       " 'type',\n",
       " 'looks',\n",
       " 'located',\n",
       " 'features',\n",
       " 'words',\n",
       " 'party',\n",
       " 'needed',\n",
       " 'size',\n",
       " 'plan',\n",
       " 'issues',\n",
       " 'else',\n",
       " 'search',\n",
       " 'black',\n",
       " 'version',\n",
       " 'News',\n",
       " 'included',\n",
       " 'Read',\n",
       " 'books',\n",
       " 'House',\n",
       " 'provided',\n",
       " 'Just',\n",
       " 'visit',\n",
       " 'couple',\n",
       " 'white',\n",
       " 'present',\n",
       " 'link',\n",
       " 'pay',\n",
       " 'coming',\n",
       " 'Some',\n",
       " 'face',\n",
       " 'kids',\n",
       " 'questions',\n",
       " 'friend',\n",
       " 'understand',\n",
       " 'however',\n",
       " 'human',\n",
       " 'became',\n",
       " 'ï',\n",
       " 'value',\n",
       " 'move',\n",
       " 'further',\n",
       " 'member',\n",
       " 'among',\n",
       " 'half',\n",
       " 'weeks',\n",
       " 'third',\n",
       " 'media',\n",
       " 'true',\n",
       " 'America',\n",
       " '»',\n",
       " 'soon',\n",
       " 'range',\n",
       " 'received',\n",
       " 'While',\n",
       " 'points',\n",
       " 'held',\n",
       " 'image',\n",
       " 'title',\n",
       " 'property',\n",
       " 'town',\n",
       " 'shows',\n",
       " 'offers',\n",
       " 'movie',\n",
       " 'happy',\n",
       " 'asked',\n",
       " 'outside',\n",
       " '£',\n",
       " 'various',\n",
       " 'writing',\n",
       " 'etc',\n",
       " 'class',\n",
       " '31',\n",
       " 'Is',\n",
       " 'contact',\n",
       " 'TV',\n",
       " 'upon',\n",
       " 'An',\n",
       " 'style',\n",
       " 'phone',\n",
       " 'stop',\n",
       " 'created',\n",
       " 'location',\n",
       " 'Click',\n",
       " 'wrote',\n",
       " 'running',\n",
       " 'art',\n",
       " 'low',\n",
       " 'child',\n",
       " 'heart',\n",
       " 'date',\n",
       " 'Do',\n",
       " 'matter',\n",
       " 'simple',\n",
       " 'address',\n",
       " 'played',\n",
       " 'similar',\n",
       " 'simply',\n",
       " 'office',\n",
       " 'enjoy',\n",
       " 'myself',\n",
       " 'saw',\n",
       " 'behind',\n",
       " 'complete',\n",
       " 'Center',\n",
       " 'card',\n",
       " 'death',\n",
       " 'problems',\n",
       " 'cannot',\n",
       " 'performance',\n",
       " 'Also',\n",
       " 'turn',\n",
       " 'action',\n",
       " 'learn',\n",
       " 'via',\n",
       " 'general',\n",
       " 'deal',\n",
       " 'includes',\n",
       " 'cost',\n",
       " 'bring',\n",
       " 'written',\n",
       " 'takes',\n",
       " 'window',\n",
       " 'Of',\n",
       " 'former',\n",
       " 'First',\n",
       " 'air',\n",
       " 'leave',\n",
       " 'likely',\n",
       " 'field',\n",
       " 'lost',\n",
       " 'areas',\n",
       " 'private',\n",
       " 'industry',\n",
       " 'store',\n",
       " 'usually',\n",
       " 'late',\n",
       " '2005',\n",
       " 'lead',\n",
       " 'Then',\n",
       " '40',\n",
       " 'clear',\n",
       " 'win',\n",
       " 'total',\n",
       " 'West',\n",
       " 'common',\n",
       " 'morning',\n",
       " 'Home',\n",
       " 'events',\n",
       " 'Day',\n",
       " 'released',\n",
       " 'Google',\n",
       " 'position',\n",
       " 'return',\n",
       " 'required',\n",
       " 'subject',\n",
       " 'account',\n",
       " 'ask',\n",
       " 'gets',\n",
       " 'comment',\n",
       " 'companies',\n",
       " 'provides',\n",
       " 'published',\n",
       " 'energy',\n",
       " 'currently',\n",
       " 'clean',\n",
       " 'President',\n",
       " 'instead',\n",
       " 'comments',\n",
       " 'final',\n",
       " 'longer',\n",
       " 'training',\n",
       " 'playing',\n",
       " '¯',\n",
       " 'period',\n",
       " 'interest',\n",
       " 'word',\n",
       " 'amount',\n",
       " 'source',\n",
       " 'rest',\n",
       " 'record',\n",
       " 'worked',\n",
       " 'Great',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'technology',\n",
       " 'wasn',\n",
       " '<',\n",
       " 'web',\n",
       " 'details',\n",
       " 'role',\n",
       " 'College',\n",
       " 'goes',\n",
       " 'Well',\n",
       " 'Park',\n",
       " 'Free',\n",
       " 'began',\n",
       " 'professional',\n",
       " 'changes',\n",
       " 'strong',\n",
       " 'meet',\n",
       " 'watch',\n",
       " 'hotel',\n",
       " 'certain',\n",
       " 'saying',\n",
       " 'sense',\n",
       " 'forward',\n",
       " 'gave',\n",
       " 'color',\n",
       " 'recently',\n",
       " 'inside',\n",
       " 'wrong',\n",
       " 'album',\n",
       " 'staff',\n",
       " 'designed',\n",
       " 'paper',\n",
       " 'amazing',\n",
       " 'drive',\n",
       " 'woman',\n",
       " 'hit',\n",
       " 'user',\n",
       " 'rights',\n",
       " 'decided',\n",
       " 'recent',\n",
       " 'key',\n",
       " 'continue',\n",
       " 'rate',\n",
       " 'posted',\n",
       " 'itself',\n",
       " 'code',\n",
       " 'favorite',\n",
       " 'seem',\n",
       " 'International',\n",
       " 'political',\n",
       " 'View',\n",
       " 'Thank',\n",
       " 'maybe',\n",
       " 'percent',\n",
       " 'remember',\n",
       " 'Why',\n",
       " '~',\n",
       " 'London',\n",
       " 'test',\n",
       " 'stuff',\n",
       " 'write',\n",
       " 'computer',\n",
       " 'cause',\n",
       " 'cut',\n",
       " 'according',\n",
       " 'built',\n",
       " 'player',\n",
       " 'popular',\n",
       " 'High',\n",
       " 'Most',\n",
       " '01',\n",
       " 'management',\n",
       " 'interesting',\n",
       " 'son',\n",
       " 'ways',\n",
       " 'education',\n",
       " 'sound',\n",
       " 'summer',\n",
       " 'ready',\n",
       " 'natural',\n",
       " 'national',\n",
       " 'software',\n",
       " '·',\n",
       " 'David',\n",
       " 'tried',\n",
       " 'yourself',\n",
       " 'players',\n",
       " 'hot',\n",
       " 'higher',\n",
       " 'policy',\n",
       " 'related',\n",
       " 'Don',\n",
       " 'miles',\n",
       " 'entire',\n",
       " 'specific',\n",
       " 'wonderful',\n",
       " 'Yes',\n",
       " 'felt',\n",
       " 'section',\n",
       " 'British',\n",
       " 'thinking',\n",
       " 'x',\n",
       " 'release',\n",
       " 'song',\n",
       " 'San',\n",
       " 'six',\n",
       " 'heard',\n",
       " 'particular',\n",
       " 'users',\n",
       " 'War',\n",
       " 'average',\n",
       " 'hear',\n",
       " 'Best',\n",
       " 'themselves',\n",
       " 'security',\n",
       " 'file',\n",
       " 'https',\n",
       " 'ones',\n",
       " '05',\n",
       " 'oil',\n",
       " 'Dr.',\n",
       " 'Facebook',\n",
       " 'See',\n",
       " 'California',\n",
       " 'cover',\n",
       " 'allow',\n",
       " 'center',\n",
       " 'Friday',\n",
       " 'terms',\n",
       " 'road',\n",
       " 'receive',\n",
       " 'board',\n",
       " 'war',\n",
       " 'material',\n",
       " 'wife',\n",
       " 'lives',\n",
       " 'sometimes',\n",
       " 'parts',\n",
       " 'UK',\n",
       " 'individual',\n",
       " 'chance',\n",
       " 'Health',\n",
       " 'items',\n",
       " 'answer',\n",
       " 'girl',\n",
       " 'himself',\n",
       " 'India',\n",
       " 'Street',\n",
       " 'walk',\n",
       " 'definitely',\n",
       " 'C',\n",
       " 'production',\n",
       " 'worth',\n",
       " 'systems',\n",
       " 'character',\n",
       " 'throughout',\n",
       " 'reviews',\n",
       " 'giving',\n",
       " 'career',\n",
       " 'increase',\n",
       " 'unique',\n",
       " 'mother',\n",
       " 'completely',\n",
       " 'picture',\n",
       " 'although',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'customers',\n",
       " 'sex',\n",
       " 'choose',\n",
       " 'parents',\n",
       " '09',\n",
       " 'involved',\n",
       " 'medical',\n",
       " 'follow',\n",
       " 'piece',\n",
       " 'articles',\n",
       " 'red',\n",
       " 'Washington',\n",
       " 'band',\n",
       " ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_embeddings_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if not word in stopwords.words('english')]\n",
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if word.isalpha()]                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vector = dict()\n",
    "word_vector_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PAD\n",
    "dummy_list = []\n",
    "dummy_list = np.zeros(200, dtype = float)\n",
    "word_vector_list.append(dummy_list)\n",
    "\n",
    "for i, word in enumerate(english_embeddings_fasttext):\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    word_vector_list.append(list(ft.get_word_vector(word)))\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "word_to_ix[\"UNK\"] = len(word_to_ix)\n",
    "\n",
    "#For UNK\n",
    "word_vector_list.append(dummy_list)\n",
    "word_vector_list = np.asarray(word_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in training_data:\n",
    "#     for word in sent:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "#             ix_to_word[word_to_ix[word]] = word\n",
    "# for tag in tags:\n",
    "#     if tag not in tag_to_ix:\n",
    "#         tag_to_ix[tag] = len(tag_to_ix)\n",
    "#         ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "# sentence= []\n",
    "# for sent in training_data:\n",
    "#      sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "#testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = []\n",
    "for sent in testing_data:\n",
    "     test_sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence= []\n",
    "for sent in training_data:\n",
    "     sentence.append(sent[:50])\n",
    "padded_sentence = sentence_to_padded_sentence(sentence, word_to_ix)\n",
    "#test_padded_sentence = sentence_to_padded_sentence(test_sentence, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,vocab_size,word_to_ix,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list):\n",
    "        super(MIMCT, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(word_vector_list))\n",
    "        \n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "        self.linear = nn.Linear(50+1,3)\n",
    "    def forward(self,x):\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        print(x)\n",
    "        embeds = self.word_embeddings(x)\n",
    "        print(embeds)\n",
    "        embeds_cnn = embeds.view(1,embeds.size(0),embeds.size(1))\n",
    "        cnn_output = self.CNN_Layers(embeds_cnn)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(x), 1, -1))\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(x), -1))\n",
    "        \n",
    "        lstm_output = self.sigmoid(tag_space)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channel = 10 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 10\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "print(len(training_data))\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "for sent in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            ix_to_word[word_to_ix[word]] = word\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_channel = 50 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 50\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "\n",
    "#Parameters for LSTM\n",
    "hidden_dim = 128\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel,vocab_size,word_to_ix,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "original_data = training_data\n",
    "training_data = padded_sentence\n",
    "# sentence1 = training_data[0]\n",
    "\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32000e+02, 1.15000e+02, 4.87369e+05, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [7.69000e+02, 3.23400e+03, 3.31050e+04, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [5.83000e+02, 4.39340e+04, 1.34870e+04, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       ...,\n",
       "       [1.87270e+04, 6.00000e+02, 0.00000e+00, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [8.48000e+02, 1.06280e+04, 1.55940e+04, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00],\n",
       "       [6.39000e+02, 6.80000e+01, 8.76600e+03, ..., 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n",
      "[1.320000e+02 1.150000e+02 4.873690e+05 7.059000e+03 1.313000e+03\n",
      " 2.000001e+06 3.606000e+04 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      "tensor([    132,     115,  487369,    7059,    1313, 2000001,   36060,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0])\n",
      "tensor([    132,     115,  487369,    7059,    1313, 2000001,   36060,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0])\n",
      "tensor([    132,     115,  487369,    7059,    1313, 2000001,   36060,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-94b622d2db08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_sequence_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-0f16b9344807>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m       \u001b[1;31m#  y = self.LSTM_Layers(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0membeds_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # running for 20 epoch\n",
    "    print(f\"Starting epoch {epoch}...\")\n",
    "    #for sentence in training_data:\n",
    "    for index,sentence in enumerate(training_data):\n",
    "        print(sentence)\n",
    "        model.zero_grad()\n",
    "        targets = tags[index]\n",
    "        sentence_in=torch.tensor(sentence, dtype=torch.long)\n",
    "        print(sentence_in)\n",
    "        targets = prepare_sequence_tags(targets, tag_to_ix)\n",
    "        print(sentence_in)\n",
    "        tag_scores = model(sentence_in)\n",
    " \n",
    "        loss = loss_function(tag_scores.cpu(), targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"hindi_tokens_translated_to_English_list.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(processed_Hindi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = test_padded_sentence\n",
    "with torch.no_grad():\n",
    "\t# this will be the file to write the outputs\n",
    "    with open(\"mymodel_output.txt\", 'w',encoding='UTF-8') as op:\n",
    "        for instance in testing_data:\n",
    "            # Convert the test sentence into a word ID tensor\n",
    "            test_sentence_in=torch.tensor(instance, dtype=torch.long)\n",
    "\n",
    "            tag_scores = model(test_sentence_in)\n",
    "\n",
    "            # Find the tag with the highest probability in each position\n",
    "            outputs = [int(np.argmax(ts)) for ts in tag_scores.cpu().detach().numpy()]\n",
    "            # Prepare the output to be written in the same format as the test file (word|tag)\n",
    "            formatted_output = ix_to_tag[outputs[0]]\n",
    "            # Write the output\n",
    "            op.write(formatted_output + '\\n')\n",
    "            \n",
    "        print(i)\n",
    "        print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
