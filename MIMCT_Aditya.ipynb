{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()\n",
    "fasttext.util.reduce_model(ft, 200)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens, tags = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "#processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 5, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6affd5f69466>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocessed_Hindi_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hindi_tokens_translated_to_English_list.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\s+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#processed_Hindi_tokens = translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 5, saw 5\n"
     ]
    }
   ],
   "source": [
    "processed_Hindi_tokens = pd.read_csv(\"hindi_tokens_translated_to_English_list.csv\", sep=\"\\s+\")\n",
    "#processed_Hindi_tokens = translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    idxs = to_ix[seq]\n",
    "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "    idxs = idxs.view(1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_padded_sentence(sentence,word_to_ix):\n",
    "    \n",
    "    # map sentences to vocab\n",
    "    sentence =  [[word_to_ix[word] for word in sent] for sent in sentence]\n",
    "    # sentence now looks like:  \n",
    "    # [[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]\n",
    "    sentence_lengths = [len(sent) for sent in sentence]\n",
    "    pad_token = word_to_ix['<PAD>']\n",
    "    longest_sent = max(sentence_lengths)\n",
    "    batch_size = len(sentence)\n",
    "    padded_sentence = np.ones((batch_size, longest_sent)) * pad_token\n",
    "    for i, x_len in enumerate(sentence_lengths):\n",
    "        sequence = sentence[i]\n",
    "        padded_sentence[i, 0:x_len] = sequence[:x_len]\n",
    "  \n",
    "    return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "word_to_ix = {\"<PAD>\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "training_data = utils.substitute_with_UNK(processed_tokens,word_to_ix)\n",
    "testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = ft.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " 'the',\n",
       " '.',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " '</s>',\n",
       " 'in',\n",
       " 'is',\n",
       " ':',\n",
       " 'I',\n",
       " 'for',\n",
       " 'that',\n",
       " ')',\n",
       " '\"',\n",
       " '(',\n",
       " 'on',\n",
       " 'with',\n",
       " 'it',\n",
       " 'you',\n",
       " 'The',\n",
       " 'was',\n",
       " 'as',\n",
       " 'are',\n",
       " 'at',\n",
       " '/',\n",
       " '’',\n",
       " 'be',\n",
       " 'by',\n",
       " \"'s\",\n",
       " 'this',\n",
       " 'have',\n",
       " 'from',\n",
       " 'or',\n",
       " '!',\n",
       " 'not',\n",
       " 'your',\n",
       " 'an',\n",
       " \"'\",\n",
       " 'but',\n",
       " '?',\n",
       " 'can',\n",
       " '-',\n",
       " 'will',\n",
       " 's',\n",
       " 'my',\n",
       " 'has',\n",
       " 'all',\n",
       " 'we',\n",
       " 'they',\n",
       " 'he',\n",
       " 'his',\n",
       " 'more',\n",
       " 'one',\n",
       " 'about',\n",
       " 'their',\n",
       " \"'t\",\n",
       " 'so',\n",
       " 'which',\n",
       " 'It',\n",
       " 'out',\n",
       " 'up',\n",
       " '...',\n",
       " 'were',\n",
       " 'had',\n",
       " 'who',\n",
       " 'like',\n",
       " ';',\n",
       " '“',\n",
       " 'our',\n",
       " 'would',\n",
       " '”',\n",
       " 'time',\n",
       " 'been',\n",
       " 'if',\n",
       " 'also',\n",
       " 'just',\n",
       " 'when',\n",
       " 'her',\n",
       " 'This',\n",
       " 'me',\n",
       " 'there',\n",
       " 'do',\n",
       " 'what',\n",
       " 'some',\n",
       " 'other',\n",
       " 'In',\n",
       " 'them',\n",
       " '–',\n",
       " '1',\n",
       " 'get',\n",
       " 'new',\n",
       " 'into',\n",
       " '&',\n",
       " 'We',\n",
       " 'than',\n",
       " 'A',\n",
       " 'no',\n",
       " 'only',\n",
       " 'first',\n",
       " 'any',\n",
       " 'its',\n",
       " 'people',\n",
       " '2',\n",
       " '$',\n",
       " 'very',\n",
       " 't',\n",
       " 'over',\n",
       " 'she',\n",
       " '%',\n",
       " 'how',\n",
       " 'make',\n",
       " 'You',\n",
       " 'said',\n",
       " 'He',\n",
       " 'two',\n",
       " 'may',\n",
       " 'know',\n",
       " 'then',\n",
       " 'see',\n",
       " 'after',\n",
       " 'most',\n",
       " 'good',\n",
       " 'years',\n",
       " 'If',\n",
       " 'these',\n",
       " 'now',\n",
       " '3',\n",
       " 'use',\n",
       " 'because',\n",
       " 'well',\n",
       " 'work',\n",
       " 'could',\n",
       " 'us',\n",
       " 'don',\n",
       " 'way',\n",
       " 'much',\n",
       " 'back',\n",
       " 'many',\n",
       " 'think',\n",
       " 'where',\n",
       " 'even',\n",
       " 'him',\n",
       " 'through',\n",
       " 'am',\n",
       " '10',\n",
       " '|',\n",
       " 'here',\n",
       " '#',\n",
       " 'made',\n",
       " 'year',\n",
       " 'should',\n",
       " '*',\n",
       " 'really',\n",
       " 'being',\n",
       " 'such',\n",
       " 'need',\n",
       " 'great',\n",
       " 'And',\n",
       " ']',\n",
       " '4',\n",
       " '[',\n",
       " '5',\n",
       " 'day',\n",
       " 'before',\n",
       " 'want',\n",
       " 'used',\n",
       " 'go',\n",
       " 'those',\n",
       " '…',\n",
       " 'But',\n",
       " 'right',\n",
       " \"'m\",\n",
       " 'take',\n",
       " '—',\n",
       " 'May',\n",
       " 'still',\n",
       " 'last',\n",
       " 'off',\n",
       " 'too',\n",
       " 'New',\n",
       " 'going',\n",
       " 'best',\n",
       " 'find',\n",
       " 'love',\n",
       " 'did',\n",
       " 'while',\n",
       " 'home',\n",
       " 'There',\n",
       " 'They',\n",
       " 'same',\n",
       " 'around',\n",
       " 'help',\n",
       " 'down',\n",
       " 'information',\n",
       " 'UTC',\n",
       " 'place',\n",
       " 'i',\n",
       " '2017',\n",
       " 'For',\n",
       " 'little',\n",
       " 'life',\n",
       " 'between',\n",
       " 'each',\n",
       " 'own',\n",
       " 'both',\n",
       " '12',\n",
       " '6',\n",
       " 'world',\n",
       " 'part',\n",
       " 'few',\n",
       " '8',\n",
       " '7',\n",
       " 'talk',\n",
       " 'As',\n",
       " 'look',\n",
       " '2012',\n",
       " 'things',\n",
       " '11',\n",
       " 'say',\n",
       " 'does',\n",
       " 'every',\n",
       " 'something',\n",
       " '2013',\n",
       " 'during',\n",
       " 'got',\n",
       " 'So',\n",
       " \"'ve\",\n",
       " 'What',\n",
       " 'since',\n",
       " 'found',\n",
       " 'long',\n",
       " 'different',\n",
       " 'says',\n",
       " '>',\n",
       " 'never',\n",
       " 'another',\n",
       " '�',\n",
       " 'Ã',\n",
       " 'better',\n",
       " '2016',\n",
       " 'using',\n",
       " '+',\n",
       " 'free',\n",
       " '20',\n",
       " 'under',\n",
       " 'three',\n",
       " 'family',\n",
       " 'She',\n",
       " 'including',\n",
       " 'That',\n",
       " 'always',\n",
       " '\\\\',\n",
       " 'next',\n",
       " 'come',\n",
       " 'without',\n",
       " 'My',\n",
       " 'again',\n",
       " '9',\n",
       " 'game',\n",
       " \"'re\",\n",
       " '15',\n",
       " '2011',\n",
       " 'When',\n",
       " 'days',\n",
       " 'set',\n",
       " '30',\n",
       " 'All',\n",
       " 'number',\n",
       " '2014',\n",
       " 'end',\n",
       " 'lot',\n",
       " 'business',\n",
       " 'sure',\n",
       " 'system',\n",
       " 'book',\n",
       " '2015',\n",
       " 'against',\n",
       " 'high',\n",
       " '=',\n",
       " '2010',\n",
       " 'must',\n",
       " 'available',\n",
       " 'To',\n",
       " 'might',\n",
       " 'show',\n",
       " 'area',\n",
       " 'Â',\n",
       " 'week',\n",
       " '00',\n",
       " 'away',\n",
       " 'team',\n",
       " 'March',\n",
       " 'name',\n",
       " 'until',\n",
       " 'April',\n",
       " 'give',\n",
       " 'thing',\n",
       " 'read',\n",
       " 'put',\n",
       " \"'ll\",\n",
       " 'On',\n",
       " 'small',\n",
       " 'school',\n",
       " 'feel',\n",
       " 'second',\n",
       " 'company',\n",
       " 'June',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'page',\n",
       " 'keep',\n",
       " 'top',\n",
       " 'why',\n",
       " 'January',\n",
       " 'site',\n",
       " '18',\n",
       " 'post',\n",
       " 'today',\n",
       " '16',\n",
       " 'within',\n",
       " '0',\n",
       " 'service',\n",
       " 'having',\n",
       " 'looking',\n",
       " 'American',\n",
       " 'point',\n",
       " 'data',\n",
       " 'though',\n",
       " '½',\n",
       " 'July',\n",
       " 'University',\n",
       " 'full',\n",
       " '14',\n",
       " 'm',\n",
       " 'able',\n",
       " 'left',\n",
       " 'With',\n",
       " 'support',\n",
       " 'United',\n",
       " '2009',\n",
       " '13',\n",
       " 'experience',\n",
       " 'room',\n",
       " '¿',\n",
       " 'water',\n",
       " 'state',\n",
       " '‘',\n",
       " 'big',\n",
       " 'order',\n",
       " 'October',\n",
       " 'No',\n",
       " 'article',\n",
       " 'case',\n",
       " 'making',\n",
       " 'enough',\n",
       " 'ago',\n",
       " 'house',\n",
       " 'September',\n",
       " '17',\n",
       " 'children',\n",
       " 'One',\n",
       " 'local',\n",
       " 'December',\n",
       " 'start',\n",
       " 'times',\n",
       " 'February',\n",
       " 'called',\n",
       " 'August',\n",
       " '25',\n",
       " 'November',\n",
       " 'ever',\n",
       " 'More',\n",
       " '2008',\n",
       " 'done',\n",
       " 'change',\n",
       " 'play',\n",
       " 'group',\n",
       " 'working',\n",
       " 'based',\n",
       " 'online',\n",
       " 'http',\n",
       " 'course',\n",
       " 'least',\n",
       " 'doesn',\n",
       " 'money',\n",
       " 'won',\n",
       " 'man',\n",
       " 'less',\n",
       " '@',\n",
       " 'important',\n",
       " 'After',\n",
       " '24',\n",
       " 'How',\n",
       " '22',\n",
       " 'try',\n",
       " 'getting',\n",
       " 'thought',\n",
       " 'public',\n",
       " 'actually',\n",
       " 'already',\n",
       " 'night',\n",
       " 'person',\n",
       " 're',\n",
       " '21',\n",
       " 'went',\n",
       " '..',\n",
       " 'story',\n",
       " 'several',\n",
       " '--',\n",
       " 'later',\n",
       " 'makes',\n",
       " 'side',\n",
       " 'list',\n",
       " 'following',\n",
       " '19',\n",
       " 'came',\n",
       " 'By',\n",
       " 'doing',\n",
       " 'power',\n",
       " 'large',\n",
       " 'season',\n",
       " 'provide',\n",
       " 'website',\n",
       " 'bit',\n",
       " 'far',\n",
       " 'real',\n",
       " 'known',\n",
       " 'took',\n",
       " 'PM',\n",
       " 'city',\n",
       " '23',\n",
       " 'let',\n",
       " 'At',\n",
       " 'along',\n",
       " 'together',\n",
       " 'per',\n",
       " '}',\n",
       " 'often',\n",
       " 'These',\n",
       " 'music',\n",
       " 'hard',\n",
       " 'God',\n",
       " 'share',\n",
       " 'Your',\n",
       " 'Our',\n",
       " 'line',\n",
       " 'process',\n",
       " 'care',\n",
       " 'others',\n",
       " 'open',\n",
       " 'John',\n",
       " 'car',\n",
       " 'services',\n",
       " 'include',\n",
       " 'food',\n",
       " 'started',\n",
       " 'easy',\n",
       " 'country',\n",
       " 'months',\n",
       " 'fact',\n",
       " 'women',\n",
       " 'yet',\n",
       " 'students',\n",
       " '2007',\n",
       " 'someone',\n",
       " 'once',\n",
       " 'live',\n",
       " 'problem',\n",
       " 'run',\n",
       " 'video',\n",
       " 've',\n",
       " 'become',\n",
       " 'government',\n",
       " 'everything',\n",
       " 'series',\n",
       " 'However',\n",
       " 'anything',\n",
       " 'four',\n",
       " 'pm',\n",
       " 'means',\n",
       " 'stay',\n",
       " '100',\n",
       " 'early',\n",
       " 'possible',\n",
       " 'past',\n",
       " 'design',\n",
       " 'quality',\n",
       " 'City',\n",
       " 'seen',\n",
       " 'States',\n",
       " 'given',\n",
       " 'pretty',\n",
       " 'State',\n",
       " 'blog',\n",
       " 'U.S.',\n",
       " \"'d\",\n",
       " 'film',\n",
       " 'nice',\n",
       " 'history',\n",
       " 'job',\n",
       " 'call',\n",
       " 'Now',\n",
       " 'kind',\n",
       " 'believe',\n",
       " 'community',\n",
       " 'needs',\n",
       " 'level',\n",
       " 'program',\n",
       " 'body',\n",
       " 'view',\n",
       " 'friends',\n",
       " 'control',\n",
       " 'comes',\n",
       " 'York',\n",
       " 'products',\n",
       " 'form',\n",
       " 'above',\n",
       " '26',\n",
       " 'School',\n",
       " 'health',\n",
       " 'minutes',\n",
       " 'probably',\n",
       " 'World',\n",
       " 'hours',\n",
       " 'fun',\n",
       " 'National',\n",
       " 'US',\n",
       " 'either',\n",
       " 'County',\n",
       " 'head',\n",
       " '28',\n",
       " 'example',\n",
       " 'product',\n",
       " 'please',\n",
       " 'close',\n",
       " 'beautiful',\n",
       " 'market',\n",
       " 'across',\n",
       " 'whole',\n",
       " 'offer',\n",
       " 'quite',\n",
       " 'trying',\n",
       " 'price',\n",
       " 'told',\n",
       " 'idea',\n",
       " 'members',\n",
       " 'light',\n",
       " '27',\n",
       " 'million',\n",
       " 'single',\n",
       " 'His',\n",
       " 'perfect',\n",
       " 'Please',\n",
       " 'hope',\n",
       " 'access',\n",
       " 'research',\n",
       " 'due',\n",
       " '2006',\n",
       " 'tell',\n",
       " 'seems',\n",
       " 'South',\n",
       " 'added',\n",
       " 'almost',\n",
       " 'current',\n",
       " 'short',\n",
       " 'bad',\n",
       " 'Not',\n",
       " '50',\n",
       " 'games',\n",
       " 'project',\n",
       " 'review',\n",
       " 'hand',\n",
       " 'email',\n",
       " 'near',\n",
       " 'below',\n",
       " 'men',\n",
       " 'wanted',\n",
       " '29',\n",
       " 'create',\n",
       " 'nothing',\n",
       " 'question',\n",
       " 'de',\n",
       " 'special',\n",
       " 'everyone',\n",
       " 'content',\n",
       " 'Posted',\n",
       " 'future',\n",
       " 'rather',\n",
       " 'check',\n",
       " 'taking',\n",
       " 'Thanks',\n",
       " 'works',\n",
       " 'From',\n",
       " 'month',\n",
       " 'front',\n",
       " 'law',\n",
       " 'space',\n",
       " 'news',\n",
       " 'Here',\n",
       " 'reason',\n",
       " 'add',\n",
       " 'isn',\n",
       " 'anyone',\n",
       " 'report',\n",
       " 'whether',\n",
       " 'major',\n",
       " 'main',\n",
       " 'young',\n",
       " 'development',\n",
       " 'North',\n",
       " 'event',\n",
       " '•',\n",
       " 'results',\n",
       " 'personal',\n",
       " 'especially',\n",
       " 'mean',\n",
       " 'issue',\n",
       " 'five',\n",
       " 'social',\n",
       " 'original',\n",
       " 'age',\n",
       " '....',\n",
       " 'born',\n",
       " 'taken',\n",
       " 'living',\n",
       " 'buy',\n",
       " 'd',\n",
       " 'll',\n",
       " 'Reply',\n",
       " 'reading',\n",
       " 'English',\n",
       " 'mind',\n",
       " 'study',\n",
       " 'building',\n",
       " 'result',\n",
       " 'type',\n",
       " 'looks',\n",
       " 'located',\n",
       " 'features',\n",
       " 'words',\n",
       " 'party',\n",
       " 'needed',\n",
       " 'size',\n",
       " 'plan',\n",
       " 'issues',\n",
       " 'else',\n",
       " 'search',\n",
       " 'black',\n",
       " 'version',\n",
       " 'News',\n",
       " 'included',\n",
       " 'Read',\n",
       " 'books',\n",
       " 'House',\n",
       " 'provided',\n",
       " 'Just',\n",
       " 'visit',\n",
       " 'couple',\n",
       " 'white',\n",
       " 'present',\n",
       " 'link',\n",
       " 'pay',\n",
       " 'coming',\n",
       " 'Some',\n",
       " 'face',\n",
       " 'kids',\n",
       " 'questions',\n",
       " 'friend',\n",
       " 'understand',\n",
       " 'however',\n",
       " 'human',\n",
       " 'became',\n",
       " 'ï',\n",
       " 'value',\n",
       " 'move',\n",
       " 'further',\n",
       " 'member',\n",
       " 'among',\n",
       " 'half',\n",
       " 'weeks',\n",
       " 'third',\n",
       " 'media',\n",
       " 'true',\n",
       " 'America',\n",
       " '»',\n",
       " 'soon',\n",
       " 'range',\n",
       " 'received',\n",
       " 'While',\n",
       " 'points',\n",
       " 'held',\n",
       " 'image',\n",
       " 'title',\n",
       " 'property',\n",
       " 'town',\n",
       " 'shows',\n",
       " 'offers',\n",
       " 'movie',\n",
       " 'happy',\n",
       " 'asked',\n",
       " 'outside',\n",
       " '£',\n",
       " 'various',\n",
       " 'writing',\n",
       " 'etc',\n",
       " 'class',\n",
       " '31',\n",
       " 'Is',\n",
       " 'contact',\n",
       " 'TV',\n",
       " 'upon',\n",
       " 'An',\n",
       " 'style',\n",
       " 'phone',\n",
       " 'stop',\n",
       " 'created',\n",
       " 'location',\n",
       " 'Click',\n",
       " 'wrote',\n",
       " 'running',\n",
       " 'art',\n",
       " 'low',\n",
       " 'child',\n",
       " 'heart',\n",
       " 'date',\n",
       " 'Do',\n",
       " 'matter',\n",
       " 'simple',\n",
       " 'address',\n",
       " 'played',\n",
       " 'similar',\n",
       " 'simply',\n",
       " 'office',\n",
       " 'enjoy',\n",
       " 'myself',\n",
       " 'saw',\n",
       " 'behind',\n",
       " 'complete',\n",
       " 'Center',\n",
       " 'card',\n",
       " 'death',\n",
       " 'problems',\n",
       " 'cannot',\n",
       " 'performance',\n",
       " 'Also',\n",
       " 'turn',\n",
       " 'action',\n",
       " 'learn',\n",
       " 'via',\n",
       " 'general',\n",
       " 'deal',\n",
       " 'includes',\n",
       " 'cost',\n",
       " 'bring',\n",
       " 'written',\n",
       " 'takes',\n",
       " 'window',\n",
       " 'Of',\n",
       " 'former',\n",
       " 'First',\n",
       " 'air',\n",
       " 'leave',\n",
       " 'likely',\n",
       " 'field',\n",
       " 'lost',\n",
       " 'areas',\n",
       " 'private',\n",
       " 'industry',\n",
       " 'store',\n",
       " 'usually',\n",
       " 'late',\n",
       " '2005',\n",
       " 'lead',\n",
       " 'Then',\n",
       " '40',\n",
       " 'clear',\n",
       " 'win',\n",
       " 'total',\n",
       " 'West',\n",
       " 'common',\n",
       " 'morning',\n",
       " 'Home',\n",
       " 'events',\n",
       " 'Day',\n",
       " 'released',\n",
       " 'Google',\n",
       " 'position',\n",
       " 'return',\n",
       " 'required',\n",
       " 'subject',\n",
       " 'account',\n",
       " 'ask',\n",
       " 'gets',\n",
       " 'comment',\n",
       " 'companies',\n",
       " 'provides',\n",
       " 'published',\n",
       " 'energy',\n",
       " 'currently',\n",
       " 'clean',\n",
       " 'President',\n",
       " 'instead',\n",
       " 'comments',\n",
       " 'final',\n",
       " 'longer',\n",
       " 'training',\n",
       " 'playing',\n",
       " '¯',\n",
       " 'period',\n",
       " 'interest',\n",
       " 'word',\n",
       " 'amount',\n",
       " 'source',\n",
       " 'rest',\n",
       " 'record',\n",
       " 'worked',\n",
       " 'Great',\n",
       " 'model',\n",
       " 'addition',\n",
       " 'technology',\n",
       " 'wasn',\n",
       " '<',\n",
       " 'web',\n",
       " 'details',\n",
       " 'role',\n",
       " 'College',\n",
       " 'goes',\n",
       " 'Well',\n",
       " 'Park',\n",
       " 'Free',\n",
       " 'began',\n",
       " 'professional',\n",
       " 'changes',\n",
       " 'strong',\n",
       " 'meet',\n",
       " 'watch',\n",
       " 'hotel',\n",
       " 'certain',\n",
       " 'saying',\n",
       " 'sense',\n",
       " 'forward',\n",
       " 'gave',\n",
       " 'color',\n",
       " 'recently',\n",
       " 'inside',\n",
       " 'wrong',\n",
       " 'album',\n",
       " 'staff',\n",
       " 'designed',\n",
       " 'paper',\n",
       " 'amazing',\n",
       " 'drive',\n",
       " 'woman',\n",
       " 'hit',\n",
       " 'user',\n",
       " 'rights',\n",
       " 'decided',\n",
       " 'recent',\n",
       " 'key',\n",
       " 'continue',\n",
       " 'rate',\n",
       " 'posted',\n",
       " 'itself',\n",
       " 'code',\n",
       " 'favorite',\n",
       " 'seem',\n",
       " 'International',\n",
       " 'political',\n",
       " 'View',\n",
       " 'Thank',\n",
       " 'maybe',\n",
       " 'percent',\n",
       " 'remember',\n",
       " 'Why',\n",
       " '~',\n",
       " 'London',\n",
       " 'test',\n",
       " 'stuff',\n",
       " 'write',\n",
       " 'computer',\n",
       " 'cause',\n",
       " 'cut',\n",
       " 'according',\n",
       " 'built',\n",
       " 'player',\n",
       " 'popular',\n",
       " 'High',\n",
       " 'Most',\n",
       " '01',\n",
       " 'management',\n",
       " 'interesting',\n",
       " 'son',\n",
       " 'ways',\n",
       " 'education',\n",
       " 'sound',\n",
       " 'summer',\n",
       " 'ready',\n",
       " 'natural',\n",
       " 'national',\n",
       " 'software',\n",
       " '·',\n",
       " 'David',\n",
       " 'tried',\n",
       " 'yourself',\n",
       " 'players',\n",
       " 'hot',\n",
       " 'higher',\n",
       " 'policy',\n",
       " 'related',\n",
       " 'Don',\n",
       " 'miles',\n",
       " 'entire',\n",
       " 'specific',\n",
       " 'wonderful',\n",
       " 'Yes',\n",
       " 'felt',\n",
       " 'section',\n",
       " 'British',\n",
       " 'thinking',\n",
       " 'x',\n",
       " 'release',\n",
       " 'song',\n",
       " 'San',\n",
       " 'six',\n",
       " 'heard',\n",
       " 'particular',\n",
       " 'users',\n",
       " 'War',\n",
       " 'average',\n",
       " 'hear',\n",
       " 'Best',\n",
       " 'themselves',\n",
       " 'security',\n",
       " 'file',\n",
       " 'https',\n",
       " 'ones',\n",
       " '05',\n",
       " 'oil',\n",
       " 'Dr.',\n",
       " 'Facebook',\n",
       " 'See',\n",
       " 'California',\n",
       " 'cover',\n",
       " 'allow',\n",
       " 'center',\n",
       " 'Friday',\n",
       " 'terms',\n",
       " 'road',\n",
       " 'receive',\n",
       " 'board',\n",
       " 'war',\n",
       " 'material',\n",
       " 'wife',\n",
       " 'lives',\n",
       " 'sometimes',\n",
       " 'parts',\n",
       " 'UK',\n",
       " 'individual',\n",
       " 'chance',\n",
       " 'Health',\n",
       " 'items',\n",
       " 'answer',\n",
       " 'girl',\n",
       " 'himself',\n",
       " 'India',\n",
       " 'Street',\n",
       " 'walk',\n",
       " 'definitely',\n",
       " 'C',\n",
       " 'production',\n",
       " 'worth',\n",
       " 'systems',\n",
       " 'character',\n",
       " 'throughout',\n",
       " 'reviews',\n",
       " 'giving',\n",
       " 'career',\n",
       " 'increase',\n",
       " 'unique',\n",
       " 'mother',\n",
       " 'completely',\n",
       " 'picture',\n",
       " 'although',\n",
       " 'St.',\n",
       " 'additional',\n",
       " 'customers',\n",
       " 'sex',\n",
       " 'choose',\n",
       " 'parents',\n",
       " '09',\n",
       " 'involved',\n",
       " 'medical',\n",
       " 'follow',\n",
       " 'piece',\n",
       " 'articles',\n",
       " 'red',\n",
       " 'Washington',\n",
       " 'band',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_embeddings_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if not word in stopwords.words('english')]\n",
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if word.isalpha()]                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vector = dict()\n",
    "word_vector_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PAD\n",
    "dummy_list = []\n",
    "dummy_list = np.zeros(200, dtype = float)\n",
    "word_vector_list.append(dummy_list)\n",
    "\n",
    "for i, word in enumerate(english_embeddings_fasttext):\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    word_vector_list.append(list(ft.get_word_vector(word)))\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "#word_to_ix[\"UNK\"] = len(word_to_ix)\n",
    "\n",
    "#For UNK\n",
    "#word_vector_list.append(dummy_list)\n",
    "word_vector_list = np.asarray(word_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del ft\n",
    "#del english_embeddings_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in training_data:\n",
    "#     for word in sent:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "#             ix_to_word[word_to_ix[word]] = word\n",
    "# for tag in tags:\n",
    "#     if tag not in tag_to_ix:\n",
    "#         tag_to_ix[tag] = len(tag_to_ix)\n",
    "#         ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "# sentence= []\n",
    "# for sent in training_data:\n",
    "#      sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "#testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = []\n",
    "for sent in testing_data:\n",
    "     test_sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence= []\n",
    "for sent in training_data:\n",
    "     sentence.append(sent[:50])\n",
    "padded_sentence = sentence_to_padded_sentence(sentence, word_to_ix)\n",
    "#test_padded_sentence = sentence_to_padded_sentence(test_sentence, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,vocab_size,word_to_ix,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list):\n",
    "        super(MIMCT, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(word_vector_list))\n",
    "        \n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "        self.linear = nn.Linear(50+1,3)\n",
    "    def forward(self,x):\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        embeds = self.word_embeddings(x)\n",
    "        embeds_cnn = embeds.view(1,embeds.size(0),embeds.size(1))\n",
    "        cnn_output = self.CNN_Layers(embeds_cnn)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(x), 1, -1))\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(x), -1))\n",
    "        \n",
    "        lstm_output = self.sigmoid(tag_space)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channel = 10 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 10\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "print(len(training_data))\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "for sent in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            ix_to_word[word_to_ix[word]] = word\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_channel = 50 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 50\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "\n",
    "#Parameters for LSTM\n",
    "hidden_dim = 128\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel,vocab_size,word_to_ix,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "original_data = training_data\n",
    "training_data = padded_sentence\n",
    "# sentence1 = training_data[0]\n",
    "\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11999"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 5\n",
    "\n",
    "# Datasets\n",
    "#partition = # IDs\n",
    "#labels = # Labels\n",
    "training_generator = torch.utils.data.DataLoader(training_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-768eb37586a8>:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = self.softmax(X)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 1.29 GiB free; 3.00 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FFAA11675A200007FFAA1167540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFAA1109C0600007FFAA1109B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FFAA111069600007FFAA110F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFAA111083A00007FFAA110F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFAA110509900007FFAA1104EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FFA4CA31FF100007FFA4CA31EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FFA4CB48AFE00007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA4CB442A500007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA44B41A3A00007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44B4000500007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44C118A000007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C228DC00007FFA44C22850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FFA449EA75500007FFA449EA720 torch_cpu.dll!at::native::zeros [<unknown file> @ <unknown line number>]\n00007FFA44CFF74000007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA447A239E00007FFA44796470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFA44B46EE400007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA447A239E00007FFA44796470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFA44C6CDBB00007FFA44C6CD10 torch_cpu.dll!at::zeros [<unknown file> @ <unknown line number>]\n00007FFA4BF9009700007FFA4BF8FEE0 torch_cuda.dll!at::native::embedding_dense_backward_cuda [<unknown file> @ <unknown line number>]\n00007FFA4CB4897300007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA4CB41AD100007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA44C0ED8200007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C223B700007FFA44C22340 torch_cpu.dll!at::embedding_dense_backward [<unknown file> @ <unknown line number>]\n00007FFA45EB8CE100007FFA45EAE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFA44B828B100007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44C0ED8200007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C223B700007FFA44C22340 torch_cpu.dll!at::embedding_dense_backward [<unknown file> @ <unknown line number>]\n00007FFA448B065600007FFA448B0620 torch_cpu.dll!at::native::embedding_backward [<unknown file> @ <unknown line number>]\n00007FFA44CE6F6500007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA45F91EBD00007FFA45EAE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFA44CDF73000007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA44C21FF400007FFA44C21EF0 torch_cpu.dll!at::embedding_backward [<unknown file> @ <unknown line number>]\n00007FFA45DFECF800007FFA45DFEBD0 torch_cpu.dll!torch::autograd::generated::EmbeddingBackward::apply [<unknown file> @ <unknown line number>]\n00007FFA45DE7E9100007FFA45DE7B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FFA4634F9BA00007FFA4634F300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FFA463503AD00007FFA4634FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFA46354FE200007FFA46354CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFA46354C4100007FFA46354BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FFA0E2F08F700007FFA0E2C9F80 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFA4634BF1400007FFA4634B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FFAFDA214C200007FFAFDA21430 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FFAFE2A703400007FFAFE2A7020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FFB0019CEC100007FFB0019CEA0 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-23b9ba0cb870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mepochLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochLoss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avg Epoch Loss is:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 1.29 GiB free; 3.00 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FFAA11675A200007FFAA1167540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFAA1109C0600007FFAA1109B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FFAA111069600007FFAA110F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFAA111083A00007FFAA110F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFAA110509900007FFAA1104EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FFA4CA31FF100007FFA4CA31EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FFA4CB48AFE00007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA4CB442A500007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA44B41A3A00007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44B4000500007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44C118A000007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C228DC00007FFA44C22850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FFA449EA75500007FFA449EA720 torch_cpu.dll!at::native::zeros [<unknown file> @ <unknown line number>]\n00007FFA44CFF74000007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA447A239E00007FFA44796470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFA44B46EE400007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA447A239E00007FFA44796470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFA44C6CDBB00007FFA44C6CD10 torch_cpu.dll!at::zeros [<unknown file> @ <unknown line number>]\n00007FFA4BF9009700007FFA4BF8FEE0 torch_cuda.dll!at::native::embedding_dense_backward_cuda [<unknown file> @ <unknown line number>]\n00007FFA4CB4897300007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA4CB41AD100007FFA4CAEE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFA44C0ED8200007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C223B700007FFA44C22340 torch_cpu.dll!at::embedding_dense_backward [<unknown file> @ <unknown line number>]\n00007FFA45EB8CE100007FFA45EAE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFA44B828B100007FFA44B2D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFA44C0ED8200007FFA44C08FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFA44C223B700007FFA44C22340 torch_cpu.dll!at::embedding_dense_backward [<unknown file> @ <unknown line number>]\n00007FFA448B065600007FFA448B0620 torch_cpu.dll!at::native::embedding_backward [<unknown file> @ <unknown line number>]\n00007FFA44CE6F6500007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA45F91EBD00007FFA45EAE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFA44CDF73000007FFA44C6D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FFA44C21FF400007FFA44C21EF0 torch_cpu.dll!at::embedding_backward [<unknown file> @ <unknown line number>]\n00007FFA45DFECF800007FFA45DFEBD0 torch_cpu.dll!torch::autograd::generated::EmbeddingBackward::apply [<unknown file> @ <unknown line number>]\n00007FFA45DE7E9100007FFA45DE7B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FFA4634F9BA00007FFA4634F300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FFA463503AD00007FFA4634FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFA46354FE200007FFA46354CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFA46354C4100007FFA46354BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FFA0E2F08F700007FFA0E2C9F80 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFA4634BF1400007FFA4634B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FFAFDA214C200007FFAFDA21430 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FFAFE2A703400007FFAFE2A7020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FFB0019CEC100007FFB0019CEA0 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # running for 20 epoch\n",
    "    print(f\"Starting epoch {epoch}...\")\n",
    "    #for sentence in training_data:\n",
    "    epochLoss = 0\n",
    "    for index,sentence in enumerate(training_data):\n",
    "        model.zero_grad()\n",
    "        targets = tags[index]\n",
    "        sentence_in=torch.tensor(sentence, dtype=torch.long)\n",
    "        targets = prepare_sequence_tags(targets, tag_to_ix)\n",
    "        tag_scores = model(sentence_in)\n",
    "        loss = loss_function(tag_scores.cpu(), targets.cpu())\n",
    "        epochLoss = epochLoss + loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Avg Epoch Loss is:\"+(epochLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"hindi_tokens_translated_to_English_list.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(processed_Hindi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = test_padded_sentence\n",
    "with torch.no_grad():\n",
    "\t# this will be the file to write the outputs\n",
    "    with open(\"mymodel_output.txt\", 'w',encoding='UTF-8') as op:\n",
    "        for instance in testing_data:\n",
    "            # Convert the test sentence into a word ID tensor\n",
    "            test_sentence_in=torch.tensor(instance, dtype=torch.long)\n",
    "\n",
    "            tag_scores = model(test_sentence_in)\n",
    "\n",
    "            # Find the tag with the highest probability in each position\n",
    "            outputs = [int(np.argmax(ts)) for ts in tag_scores.cpu().detach().numpy()]\n",
    "            # Prepare the output to be written in the same format as the test file (word|tag)\n",
    "            formatted_output = ix_to_tag[outputs[0]]\n",
    "            # Write the output\n",
    "            op.write(formatted_output + '\\n')\n",
    "            \n",
    "        print(i)\n",
    "        print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
