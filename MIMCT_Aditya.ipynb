{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        if i == 50:\n",
    "            break\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "100\n",
      "najayaz\n",
      "1-illegitimate\n",
      "100\n",
      "haraami\n",
      "1-bastard\n",
      "80\n",
      "hain\n",
      "1-buffalo\n",
      "75\n",
      "adat\n",
      "1-breed\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "80\n",
      "hotay\n",
      "1-donkey\n",
      "77\n",
      "bharosa\n",
      "1-pussy\n",
      "75\n",
      "the\n",
      "1-idiot\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "mc\n",
      "1-mother fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "91\n",
      "ghuss\n",
      "1-fuck\n",
      "75\n",
      "maar\n",
      "1-mother’s\n",
      "75\n",
      "kaat\n",
      "1-breed\n",
      "80\n",
      "sawal\n",
      "1-sister’s husband\n",
      "75\n",
      "iss\n",
      "1-bug\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "maan\n",
      "1-mother’s\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "katy\n",
      "1-cut\n",
      "75\n",
      "jaty\n",
      "1-breed\n",
      "80\n",
      "khtay\n",
      "1-donkey\n",
      "75\n",
      "ghus\n",
      "1-sucker\n",
      "75\n",
      "haramilog\n",
      "1-bastard\n",
      "80\n",
      "payda\n",
      "1-fat bitch\n",
      "77\n",
      "chinta\n",
      "1-whore\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "80\n",
      "gande\n",
      "1-asshole\n",
      "75\n",
      "kha\n",
      "1-donkey\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "gandi\n",
      "1-hooker\n",
      "75\n",
      "krte\n",
      "1-dog\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "75\n",
      "bnd\n",
      "1-semi-dick\n",
      "80\n",
      "br\n",
      "1-pussy\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "haat\n",
      "1-breed\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "83\n",
      "chopra\n",
      "1-fucker\n",
      "80\n",
      "khola\n",
      "1-donkey\n",
      "80\n",
      "gadhe\n",
      "1-boobs\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "padai\n",
      "1-fat bitch\n",
      "89\n",
      "bhot\n",
      "1-ghost\n",
      "75\n",
      "sahi\n",
      "1-stinking\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "sali\n",
      "1-stinking\n",
      "92\n",
      "chutiya\n",
      "1-fucker\n",
      "75\n",
      "kaam\n",
      "1-cut\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "chutiyapa\n",
      "1-fucker\n",
      "91\n",
      "kuttey\n",
      "1-dog\n",
      "100\n",
      "tatti\n",
      "1-shit\n",
      "75\n",
      "gali\n",
      "1-nigger\n",
      "75\n",
      "dna\n",
      "1-stick\n",
      "75\n",
      "aata\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "77\n",
      "chudian\n",
      "1-fucker\n",
      "89\n",
      "shadi\n",
      "1-stinking\n",
      "75\n",
      "baat\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "mai\n",
      "1-son-in-law\n",
      "89\n",
      "bana\n",
      "1-semi-dick\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "80\n",
      "padha\n",
      "1-fat bitch\n",
      "100\n",
      "katua\n",
      "1-dick cut\n",
      "80\n",
      "kutte\n",
      "1-dog\n",
      "['रॉड', 'तुम्हारे', 'illegitimate', 'बाप', 'है', 'area', 'है', 'की', 'स्वाहा', 'की', 'नहीं', 'हैट्स', 'सांता', 'bastard', 'आज़ाद', 'माल्क', 'है', 'सब', 'जयगढ़', 'जाएगा']\n",
      "['first', 'stage', 'par', 'डेटस्कलैंड', 'टूहे', 'श्रेय', 'ओवर', 'से']\n",
      "['guys', 'counter', 'modi', 'govt', 'decisions', 'fact', 'black', 'money', 'cleanup', 'stand', 'taken', 'many', 'discussions', 'news', 'channel', 'individuals', 'meetings', 'much', 'efforts', 'made', 'media', 'make', 'people', 'scared', 'provoke']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pakistan',\n",
       "  'comprised',\n",
       "  'fake',\n",
       "  'muslims',\n",
       "  'know',\n",
       "  'meaning',\n",
       "  'unity',\n",
       "  'imposes',\n",
       "  'thoughts',\n",
       "  'others',\n",
       "  'rascals',\n",
       "  'gathered'],\n",
       " ['समन',\n",
       "  'और',\n",
       "  'आमिराह',\n",
       "  'की',\n",
       "  'कुंती',\n",
       "  'मोबाइल',\n",
       "  'release',\n",
       "  'हुए',\n",
       "  'जो',\n",
       "  'आँधी',\n",
       "  'me',\n",
       "  'dub',\n",
       "  'गए',\n",
       "  'बिकल',\n",
       "  'भाटकर',\n",
       "  'media'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens[3], processed_Hindi_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.linear = nn.Linear(input_channel+1,3)\n",
    "    def forward(self,x):\n",
    "        cnn_output = self.CNN_Layers(x)\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(lstm_out.size(1), -1))\n",
    "        lstm_output = F.log_softmax(tag_space, dim=1)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        \n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        \n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 20\n",
    "embedding_dim = 200\n",
    "output_channel = 20\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22837767, -0.5004736 ,  0.03795812,  0.2769011 ,  0.06002353,\n",
       "        0.2443759 ,  0.24667862, -0.5268596 ,  0.3109252 , -0.09721513,\n",
       "        0.08536617,  0.02022322,  0.38826668,  0.0800725 ,  0.01506799,\n",
       "        0.18854944, -0.02170865,  0.22188829, -0.01396991, -0.09093815,\n",
       "        0.07118342, -0.21627381, -0.03814943, -0.09304668, -0.08171362,\n",
       "       -0.00936724,  0.00458454,  0.09245228,  0.03126896, -0.22682981,\n",
       "        0.299948  , -0.07215133, -0.04840038, -0.30835494, -0.27917308,\n",
       "        0.17136611,  0.05805624, -0.21451256,  0.07856762, -0.12793331,\n",
       "       -0.27447063,  0.04271372,  0.12385953,  0.03228279, -0.4318502 ,\n",
       "       -0.00466134,  0.12952508,  0.12921299, -0.4957934 , -0.2114573 ,\n",
       "       -0.2827603 , -0.16331264, -0.48308343,  0.66749406, -0.4567007 ,\n",
       "        0.09083649,  0.3733607 , -0.1783615 ,  0.07900339,  0.11910054,\n",
       "       -0.18478079, -0.3049171 ,  0.18541874,  0.03618794, -0.00326595,\n",
       "        0.06624629,  0.0454898 , -0.02409362, -0.16452976,  0.02812767,\n",
       "       -0.03078479, -0.20783675, -0.16054147,  0.15303516, -0.0255521 ,\n",
       "       -0.0013181 ,  0.6520695 ,  0.06604471,  0.05321269, -0.04665343,\n",
       "       -0.12789695,  0.20566657, -0.17044908, -0.14685234, -0.01704394,\n",
       "        0.12047333,  0.14537469, -0.17362867,  0.52035034,  0.17230222,\n",
       "       -0.23933901, -0.2693685 ,  0.04188803, -0.10543806,  0.25154972,\n",
       "       -0.13223879,  0.10018335, -0.00123936, -0.09952165, -0.04815073,\n",
       "       -0.07007961, -0.06899633,  0.02319484, -0.0352538 , -0.0304815 ,\n",
       "       -0.34932822, -0.33299145,  0.19273281,  0.29741657, -0.05413064,\n",
       "       -0.2832284 , -0.3406367 ,  0.19894078, -0.16747457,  0.35970956,\n",
       "        0.3073346 , -0.31861722,  0.02498828,  0.11626031,  0.01996399,\n",
       "       -0.00478921,  0.0173524 ,  0.05792423,  0.00420092, -0.08506414,\n",
       "       -0.64927006,  0.00175374,  0.1491171 , -0.11759008, -0.13777864,\n",
       "       -0.22007838,  0.03952278,  0.07920182, -0.3339041 , -0.20209748,\n",
       "        0.2536904 ,  0.3064118 ,  0.21395127, -0.00783767, -0.04906023,\n",
       "       -0.01179575,  0.11673953, -0.00716219,  0.01952332, -0.13982141,\n",
       "        0.02539791,  0.3848009 ,  0.08534929,  0.1409253 ,  0.1142863 ,\n",
       "       -0.15729912, -0.2110354 ,  0.19420436, -0.1676652 , -0.04315299,\n",
       "        0.10405324, -0.03459496, -0.21469253,  0.00758332, -0.20575929,\n",
       "       -0.15528953,  0.20097217,  0.15408778,  0.01555694, -0.11711626,\n",
       "       -0.27376458,  0.31205243,  0.2183661 ,  0.05779367, -0.1328975 ,\n",
       "        0.07399072, -0.09814265,  0.04438574, -0.03056746, -0.00898725,\n",
       "       -0.17422172,  0.11955601,  0.03632375, -0.17525393,  0.05055578,\n",
       "       -0.02876653, -0.20269635,  0.08359186, -0.07555664, -0.0584923 ,\n",
       "       -0.11586109, -0.0228888 ,  0.09688257,  0.07393039, -0.16506368,\n",
       "       -0.00711951, -0.0094192 ,  0.0636051 , -0.26395667, -0.01573581,\n",
       "        0.21366417, -0.09114897, -0.09307356, -0.16196229, -0.08420379],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector(\"in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "Requirement already satisfied: pybind11>=2.2 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (2.6.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (49.2.0.post20200714)\n",
      "Requirement already satisfied: numpy in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (1.18.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'done'\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-win_amd64.whl size=233272 sha256=5af58df46e233467acbd658fd3f2d859e451d89f36ea53f455aec9f824e1237d\n",
      "  Stored in directory: c:\\users\\addyj\\appdata\\local\\pip\\cache\\wheels\\93\\61\\2a\\c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: fasttext 0.9.2 requires pybind11>=2.2, which is not installed.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()\n",
    "fasttext.util.reduce_model(ft, 200)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLength = max(map(len, processed_tokens))\n",
    "maxLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_list = []\n",
    "for i, word in enumerate(data_vector_English):\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    word_vector_list.append(list(data_vector[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for LSTM\n",
    "hidden_dim = 128\n",
    "dropout = 0.25\n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "#The length vocabulary formed after translating the Hindi and Hinglish tokens to English along with English Tokens \n",
    "vocab_size = len(set(HE_dict.values())) + len(set(H_dict_F.values())) + len(set(EH_dict.keys())) + len(set(EH_dict_F.keys()))\n",
    "\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear).to(cuda)\n",
    "loss_function = nn.NLLLoss()\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "target = torch.tensor([1])\n",
    "output = model(input1)\n",
    "loss = loss_function(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
