{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n"
     ]
    }
   ],
   "source": [
    "f = \"english/agr_en_train.csv\"\n",
    "\n",
    "# preprocessing english tweets.\n",
    "#ingesting english csv file\n",
    "df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "\n",
    "df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "\n",
    "comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "tags = np.asarray(df['annotation'])\n",
    "print((len(comments)))\n",
    "print(len(tags))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "processed_tokens = []\n",
    "for comment in comments:\n",
    "#    comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "    comment = comment.lower()   #lower casing each tweets\n",
    "    Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "    URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "    new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "    sentence = []\n",
    "    for word in new_words:\n",
    "        if word not in stop_words:           #checking for stop words on each sentence\n",
    "            sentence.append(word)\n",
    "    processed_tokens.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First stage par dus jootey khaye Grover  se\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'media',\n",
       " 'reports',\n",
       " 'k',\n",
       " 'mutabiq',\n",
       " 'jawan',\n",
       " 'patroling',\n",
       " 'kur',\n",
       " 'rahy',\n",
       " 'thy',\n",
       " 'jb',\n",
       " 'bat',\n",
       " 'ne',\n",
       " 'ghuss',\n",
       " 'k',\n",
       " 'ko',\n",
       " 'maar',\n",
       " 'k',\n",
       " 'sur',\n",
       " 'kaat',\n",
       " 'liye',\n",
       " 'yahan',\n",
       " 'yeh',\n",
       " 'sawal',\n",
       " 'bunta',\n",
       " 'hai',\n",
       " 'jo',\n",
       " 'koi',\n",
       " 'indian',\n",
       " 'nehi',\n",
       " 'dega',\n",
       " 'k',\n",
       " 'baaki',\n",
       " 'buzdil',\n",
       " 'jawan',\n",
       " 'iss',\n",
       " 'doran',\n",
       " 'apni',\n",
       " 'geeli',\n",
       " 'pents',\n",
       " 'lekar',\n",
       " 'kahan',\n",
       " 'chup',\n",
       " 'gaye',\n",
       " 'thy',\n",
       " 'chalo',\n",
       " 'maan',\n",
       " 'lia',\n",
       " 'k',\n",
       " 'pak',\n",
       " 'army',\n",
       " 'ne',\n",
       " 'chup',\n",
       " 'k',\n",
       " 'attack',\n",
       " 'kia',\n",
       " 'hoga',\n",
       " 'magar',\n",
       " 'fouji',\n",
       " 'muqabla',\n",
       " 'kurty',\n",
       " 'toh',\n",
       " 'sir',\n",
       " 'toh',\n",
       " 'na',\n",
       " 'katy',\n",
       " 'jaty',\n",
       " 'becharon',\n",
       " 'k',\n",
       " 'bhartiyo',\n",
       " 'jawab',\n",
       " 'do',\n",
       " 'buzdil',\n",
       " 'kahan',\n",
       " 'bhag',\n",
       " 'gaye',\n",
       " 'thy',\n",
       " 'tb',\n",
       " 'esay',\n",
       " 'khtay',\n",
       " 'hai',\n",
       " 'ghus',\n",
       " 'ky',\n",
       " 'marna']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "\n",
    "Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "hindi_tags = np.asarray(df1['annotation'])\n",
    "print((hindi_comments[1])) \n",
    "processed_Hindi_tokens = []\n",
    "for comment in hindi_comments:\n",
    "#    comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "    comment = comment.lower()   #lower casing each tweets\n",
    "    Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "    URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "    Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "    if (isEnglish(Emoji_removal) == True):\n",
    "        Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "    processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "processed_Hindi_tokens[0]\n",
    "processed_Hindi_tokens[11]\n",
    "processed_Hindi_tokens[6]\n",
    "processed_Hindi_tokens[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4a05a45898a1>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],sep='\\s')\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 2 fields in line 1359, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4a05a45898a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#-----------Transliteration and translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtransliteration_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"transliterations.hi-en.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mt_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransliteration_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Hinglish'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Hindi'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hinglish'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hinglish'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hindi'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hindi'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2456\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2458\u001b[1;33m         \u001b[0malldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rows_to_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2459\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exclude_implicit_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_rows_to_cols\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m   3111\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\". \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreason\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3113\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alert_malformed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3115\u001b[0m         \u001b[1;31m# see gh-13320\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_alert_malformed\u001b[1;34m(self, msg, row_num)\u001b[0m\n\u001b[0;32m   2870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2872\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2873\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2874\u001b[0m             \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Skipping line {row_num}: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Expected 2 fields in line 1359, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "#-----------Transliteration and translation\n",
    "transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "t_dict = np.asarray(t_dict)\n",
    "\n",
    "#--------------profanity dictionary\n",
    "profanity_dict = \"ProfanityText.txt\"\n",
    "P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "P_dict['English'] = P_dict['English'].str.strip()\n",
    "P_dict = np.asarray(P_dict)\n",
    "\n",
    "\n",
    "print(t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens[4]\n",
    "for i in range(0,len(processed_Hindi_tokens)):\n",
    "    print(i)\n",
    "    for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "        flag = 0\n",
    "        Str1 = (processed_Hindi_tokens[i][j])\n",
    "        max_ratio = 60\n",
    "        max_ratio_P = 75   #needs to be adjusted\n",
    "        if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "            continue;\n",
    "        for l in range(0,len(P_dict)):\n",
    "            Str2 = P_dict[l][0]\n",
    "            Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "            if (Ratiostr1 >= max_ratio_P):\n",
    "                print(Ratiostr1)\n",
    "                max_ratio_P = Ratiostr1\n",
    "                processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                flag = 1 \n",
    "                print(flag)\n",
    "                break;\n",
    "        for p in EH_dict_F:\n",
    "            RAtionstr1 = fuzz.ratio(Str1,str(p))\n",
    "            if(Ratiostr1 >= 98):\n",
    "                flag = 1\n",
    "                break;\n",
    "        if (flag == 1):\n",
    "            continue;\n",
    "        \n",
    "        else:\n",
    "            for k in range(0,len(t_dict)):\n",
    "                Str2 = t_dict[k][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 > max_ratio):\n",
    "                    max_ratio = Ratiostr1\n",
    "                    processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "processed_Hindi_tokens[0]\n",
    "processed_Hindi_tokens[1]\n",
    "(processed_tokens[12]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "\n",
    "Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "\n",
    "H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "H_dict['English'] = H_dict['English'].str.strip()\n",
    "H_hindi = np.asarray(H_dict['Hindi'])\n",
    "H_english = np.asarray(H_dict['English'])\n",
    "\n",
    "HE_dict = dict(zip(H_hindi,H_english))\n",
    "H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "\n",
    "for i in range(0,len(processed_Hindi_tokens)):\n",
    "    print(i)\n",
    "    for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "        Str = processed_Hindi_tokens[i][j]\n",
    "        if(Str in HE_dict):\n",
    "            processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "        elif(Str in H_dict_F):\n",
    "            processed_Hindi_tokens[i][j] = H_dict_F[Str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------MODEL\n",
    "\n",
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        #create a sequential for LSTM.\n",
    "        self.LSTM_Layers = nn.Sequential(\n",
    "            nn.LSTM(input_channel, hidden_dim, output_channel,dropout),\n",
    "            nn.Linear(input_channel, hidden_dim, output_channel),\n",
    "            nn.Linear(input_channel, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.CNN_Layers(x)\n",
    "        y = self.LSTM_Layers(x)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function, \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for LSTM\n",
    "hidden_dim = 64\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,kernel_size,feature_linear)\n",
    "\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "output = model(input1)\n",
    "#create the loss cretirion and training loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "m = nn.Conv1d(1, 2,1,stride=2)\n",
    "input1 = torch.randn(10)\n",
    "cnn1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n",
    "input1 = input1.unsqueeze(0).unsqueeze(0)\n",
    "input1.shape\n",
    "cnn1d_2(input1)\n",
    "\n",
    "\n",
    "\n",
    "#------------CNN_Class----------------#\n",
    "kernel_size = [4,3,2]\n",
    "embedding_dim = 10\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "\n",
    "Layer1 = nn.Conv1d(16, 16, 4, stride=1)\n",
    "Layer2 = nn.Conv1d(16, 16, 3, stride=1)\n",
    "Layer3 = nn.Conv1d(16, 16, 2, stride=1)\n",
    "Dropout_layer = nn.Dropout(p=0.25)\n",
    "dense_layer = nn.Linear(Feature_layer3, 3)\n",
    "nn.Flatten\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 10\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "output = Layer1(input1)\n",
    "output1 = Layer2(output)\n",
    "output2 = Layer3(output1)\n",
    "output3 = Dropout_layer(output2)\n",
    "output4 = dense_layer(output3)\n",
    "nn.flatten\n",
    "\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],[0.0,  1.0],[0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)\n",
    "m.weight\n",
    "\n",
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.CNN_Layers(x)\n",
    "        return x\n",
    "\n",
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,kernel_size,feature_linear)\n",
    "\n",
    "output = model(input1)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "m = nn.Sequential( nn.Conv1d(input_channel,16,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "test_input = input1\n",
    "output = m(test_input)\n",
    "nn.flatten\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
