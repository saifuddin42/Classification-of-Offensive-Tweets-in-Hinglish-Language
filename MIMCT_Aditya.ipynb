{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()\n",
    "fasttext.util.reduce_model(ft, 200)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens, tags = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "#processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_Hindi_tokens = pd.read_csv(\"hindi_tokens_translated_to_English_list.csv\", sep=\"\\s+\")\n",
    "#processed_Hindi_tokens = translate_hindi_back_to_English(processed_Hindi_tokens, HE_dict, H_dict_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long).to(cuda)\n",
    "\n",
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    idxs = to_ix[seq]\n",
    "    idxs = torch.tensor(idxs, dtype=torch.long).to(cuda)\n",
    "    idxs = idxs.view(1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_padded_sentence(sentence,word_to_ix):\n",
    "    \n",
    "    # map sentences to vocab\n",
    "    sentence =  [[word_to_ix[word] for word in sent] for sent in sentence]\n",
    "    # sentence now looks like:  \n",
    "    # [[1, 2, 3, 4, 5, 6, 7], [8, 8], [7, 9]]\n",
    "    sentence_lengths = [len(sent) for sent in sentence]\n",
    "    pad_token = word_to_ix['<PAD>']\n",
    "    longest_sent = max(sentence_lengths)\n",
    "    batch_size = len(sentence)\n",
    "    padded_sentence = np.ones((batch_size, longest_sent)) * pad_token\n",
    "    for i, x_len in enumerate(sentence_lengths):\n",
    "        sequence = sentence[i]\n",
    "        padded_sentence[i, 0:x_len] = sequence[:x_len]\n",
    "  \n",
    "    return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "word_to_ix = {\"<PAD>\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "training_data = utils.substitute_with_UNK(processed_tokens,word_to_ix)\n",
    "testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = ft.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if not word in stopwords.words('english')]\n",
    "english_embeddings_fasttext = [word.lower() for word in english_embeddings_fasttext if word.isalpha()]                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vector = dict()\n",
    "word_vector_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PAD\n",
    "dummy_list = []\n",
    "dummy_list = np.zeros(200, dtype = float)\n",
    "word_vector_list.append(dummy_list)\n",
    "\n",
    "for i, word in enumerate(english_embeddings_fasttext):\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    word_vector_list.append(list(ft.get_word_vector(word)))\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "#word_to_ix[\"UNK\"] = len(word_to_ix)\n",
    "\n",
    "#For UNK\n",
    "#word_vector_list.append(dummy_list)\n",
    "word_vector_list = np.asarray(word_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ft\n",
    "del english_embeddings_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in training_data:\n",
    "#     for word in sent:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "#             ix_to_word[word_to_ix[word]] = word\n",
    "# for tag in tags:\n",
    "#     if tag not in tag_to_ix:\n",
    "#         tag_to_ix[tag] = len(tag_to_ix)\n",
    "#         ix_to_tag[tag_to_ix[tag]] = tag\n",
    "\n",
    "# sentence= []\n",
    "# for sent in training_data:\n",
    "#      sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)\n",
    "#testing_data = utils.substitute_with_UNK_for_TEST(processed_tokens,word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = []\n",
    "for sent in testing_data:\n",
    "     test_sentence.append(sent[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence= []\n",
    "for sent in training_data:\n",
    "     sentence.append(sent[:50])\n",
    "padded_sentence = sentence_to_padded_sentence(sentence, word_to_ix)\n",
    "#test_padded_sentence = sentence_to_padded_sentence(test_sentence, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,vocab_size,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list):\n",
    "        super(MIMCT, self).__init__()\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(word_vector_list))\n",
    "        \n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "        self.linear = nn.Linear(50+1,3)\n",
    "    def forward(self,x):\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        embeds = self.word_embeddings(x)\n",
    "        embeds_cnn = embeds.view(1,embeds.size(0),embeds.size(1))\n",
    "        cnn_output = self.CNN_Layers(embeds_cnn)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(x), 1, -1))\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(x), -1))\n",
    "        \n",
    "        lstm_output = self.sigmoid(tag_space)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.50266868, -0.28379977, -0.11156563, ..., -0.0098572 ,\n",
       "        -0.07419434,  0.03370283],\n",
       "       [ 0.16699062, -0.11859183,  0.02268529, ..., -0.08538958,\n",
       "        -0.01879141,  0.01088265],\n",
       "       ...,\n",
       "       [-0.90777683, -0.60321951,  0.03140516, ...,  0.04641619,\n",
       "         0.04584672,  0.12390068],\n",
       "       [-0.1016451 , -0.04361457,  0.00473971, ...,  0.01216098,\n",
       "        -0.01749018,  0.00882431],\n",
       "       [-0.1368283 , -0.04461518, -0.01831819, ...,  0.05250729,\n",
       "        -0.12551871, -0.0221833 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channel = 10 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 10\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "print(len(training_data))\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "for sent in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            ix_to_word[word_to_ix[word]] = word\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 1.29 GiB free; 3.00 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ace91b74eab8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#recurrent_dropout = 0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIMCT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_channel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_channel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_vector_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#Adam Optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 6.00 GiB total capacity; 2.99 GiB already allocated; 1.29 GiB free; 3.00 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_channel = 50 #vocab size\n",
    "vocab_size = len(word_to_ix) \n",
    "embedding_dim = 200 \n",
    "output_channel = 50\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "\n",
    "#Parameters for LSTM\n",
    "hidden_dim = 128\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel,vocab_size,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list).to(cuda)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "original_data = training_data\n",
    "training_data = padded_sentence\n",
    "# sentence1 = training_data[0]\n",
    "\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dataset = np.concatenate((training_data, tags))\n",
    "np.asarray(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "import math\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 5\n",
    "total_samples = len(training_data)\n",
    "num_iter = math.ceil(total_samples/100)\n",
    "\n",
    "# Datasets\n",
    "#partition = # IDs\n",
    "#labels = # Labels\n",
    "training_generator = torch.utils.data.DataLoader(training_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-ad44990ab5d5>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_in=torch.tensor(sentence, dtype=torch.long).to(cuda)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ad44990ab5d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0msentence_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_sequence_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mepochLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochLoss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-768eb37586a8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m       \u001b[1;31m#  y = self.LSTM_Layers(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0membeds_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mcnn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCNN_Layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds_cnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mD:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):  # running for 20 epoch\n",
    "    epochLoss = 0\n",
    "    for i ,data in enumerate(training_generator):    \n",
    "        for index,sentence in enumerate(data):\n",
    "            model.zero_grad()\n",
    "            targets = tags[index + (i * num_iter)]\n",
    "            sentence_in=torch.tensor(sentence, dtype=torch.long).to(cuda)\n",
    "            targets = prepare_sequence_tags(targets, tag_to_ix)\n",
    "            tag_scores = model(sentence_in)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            epochLoss = epochLoss + loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Avg Epoch Loss is: \",epochLoss / max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):  # running for 20 epoch\n",
    "    print(f\"Starting epoch {epoch}...\")\n",
    "    #for sentence in training_data:\n",
    "    epochLoss = 0\n",
    "    for index,sentence in enumerate(training_data):\n",
    "        model.zero_grad()\n",
    "        targets = tags[index]\n",
    "        sentence_in=torch.tensor(sentence, dtype=torch.long)\n",
    "        targets = prepare_sequence_tags(targets, tag_to_ix)\n",
    "        tag_scores = model(sentence_in)\n",
    "        loss = loss_function(tag_scores.cpu(), targets.cpu())\n",
    "        epochLoss = epochLoss + loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Avg Epoch Loss is: \",epochLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Hindi_tokens\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"hindi_tokens_translated_to_English_list.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(processed_Hindi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = test_padded_sentence\n",
    "with torch.no_grad():\n",
    "\t# this will be the file to write the outputs\n",
    "    with open(\"mymodel_output.txt\", 'w',encoding='UTF-8') as op:\n",
    "        for instance in testing_data:\n",
    "            # Convert the test sentence into a word ID tensor\n",
    "            test_sentence_in=torch.tensor(instance, dtype=torch.long)\n",
    "\n",
    "            tag_scores = model(test_sentence_in)\n",
    "\n",
    "            # Find the tag with the highest probability in each position\n",
    "            outputs = [int(np.argmax(ts)) for ts in tag_scores.cpu().detach().numpy()]\n",
    "            # Prepare the output to be written in the same format as the test file (word|tag)\n",
    "            formatted_output = ix_to_tag[outputs[0]]\n",
    "            # Write the output\n",
    "            op.write(formatted_output + '\\n')\n",
    "            \n",
    "        print(i)\n",
    "        print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
