{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        if i == 50:\n",
    "            break\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in d:\\softwares\\anaconda\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (2.6.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (49.2.0.post20200714)\n",
      "Requirement already satisfied: numpy in d:\\softwares\\anaconda\\lib\\site-packages (from fasttext) (1.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()\n",
    "fasttext.util.reduce_model(ft, 200)\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "100\n",
      "najayaz\n",
      "1-illegitimate\n",
      "100\n",
      "haraami\n",
      "1-bastard\n",
      "80\n",
      "hain\n",
      "1-buffalo\n",
      "75\n",
      "adat\n",
      "1-breed\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "80\n",
      "hotay\n",
      "1-donkey\n",
      "77\n",
      "bharosa\n",
      "1-pussy\n",
      "75\n",
      "the\n",
      "1-idiot\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "mc\n",
      "1-mother fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "91\n",
      "ghuss\n",
      "1-fuck\n",
      "75\n",
      "maar\n",
      "1-mother’s\n",
      "75\n",
      "kaat\n",
      "1-breed\n",
      "80\n",
      "sawal\n",
      "1-sister’s husband\n",
      "75\n",
      "iss\n",
      "1-bug\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "maan\n",
      "1-mother’s\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "katy\n",
      "1-cut\n",
      "75\n",
      "jaty\n",
      "1-breed\n",
      "80\n",
      "khtay\n",
      "1-donkey\n",
      "75\n",
      "ghus\n",
      "1-sucker\n",
      "75\n",
      "haramilog\n",
      "1-bastard\n",
      "80\n",
      "payda\n",
      "1-fat bitch\n",
      "77\n",
      "chinta\n",
      "1-whore\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "80\n",
      "gande\n",
      "1-asshole\n",
      "75\n",
      "kha\n",
      "1-donkey\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "gandi\n",
      "1-hooker\n",
      "75\n",
      "krte\n",
      "1-dog\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "75\n",
      "bnd\n",
      "1-semi-dick\n",
      "80\n",
      "br\n",
      "1-pussy\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "haat\n",
      "1-breed\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "83\n",
      "chopra\n",
      "1-fucker\n",
      "80\n",
      "khola\n",
      "1-donkey\n",
      "80\n",
      "gadhe\n",
      "1-boobs\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "padai\n",
      "1-fat bitch\n",
      "89\n",
      "bhot\n",
      "1-ghost\n",
      "75\n",
      "sahi\n",
      "1-stinking\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "sali\n",
      "1-stinking\n",
      "92\n",
      "chutiya\n",
      "1-fucker\n",
      "75\n",
      "kaam\n",
      "1-cut\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "chutiyapa\n",
      "1-fucker\n",
      "91\n",
      "kuttey\n",
      "1-dog\n",
      "100\n",
      "tatti\n",
      "1-shit\n",
      "75\n",
      "gali\n",
      "1-nigger\n",
      "75\n",
      "dna\n",
      "1-stick\n",
      "75\n",
      "aata\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "77\n",
      "chudian\n",
      "1-fucker\n",
      "89\n",
      "shadi\n",
      "1-stinking\n",
      "75\n",
      "baat\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "mai\n",
      "1-son-in-law\n",
      "89\n",
      "bana\n",
      "1-semi-dick\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "80\n",
      "padha\n",
      "1-fat bitch\n",
      "100\n",
      "katua\n",
      "1-dick cut\n",
      "80\n",
      "kutte\n",
      "1-dog\n",
      "['रॉड', 'तुम्हारे', 'illegitimate', 'बाप', 'है', 'area', 'है', 'की', 'स्वाहा', 'की', 'नहीं', 'हैट्स', 'सांता', 'bastard', 'आज़ाद', 'माल्क', 'है', 'सब', 'जयगढ़', 'जाएगा']\n",
      "['first', 'stage', 'par', 'डेटस्कलैंड', 'टूहे', 'श्रेय', 'ओवर', 'से']\n",
      "['guys', 'counter', 'modi', 'govt', 'decisions', 'fact', 'black', 'money', 'cleanup', 'stand', 'taken', 'many', 'discussions', 'news', 'channel', 'individuals', 'meetings', 'much', 'efforts', 'made', 'media', 'make', 'people', 'scared', 'provoke']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens, tags = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pakistan',\n",
       "  'comprised',\n",
       "  'fake',\n",
       "  'muslims',\n",
       "  'know',\n",
       "  'meaning',\n",
       "  'unity',\n",
       "  'imposes',\n",
       "  'thoughts',\n",
       "  'others',\n",
       "  'rascals',\n",
       "  'gathered'],\n",
       " ['समन',\n",
       "  'और',\n",
       "  'आमिराह',\n",
       "  'की',\n",
       "  'कुंती',\n",
       "  'मोबाइल',\n",
       "  'release',\n",
       "  'हुए',\n",
       "  'जो',\n",
       "  'आँधी',\n",
       "  'me',\n",
       "  'dub',\n",
       "  'गए',\n",
       "  'बिकल',\n",
       "  'भाटकर',\n",
       "  'media'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens[3], processed_Hindi_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        self.word_embeddings = nn.Embedding(input_channel, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.linear = nn.Linear(input_channel+1,3)\n",
    "    def forward(self,x, x_padded):\n",
    "        cnn_output = self.CNN_Layers(x_padded)\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        embeds = self.word_embeddings(x)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(lstm_out.size(1), -1))\n",
    "        lstm_output = F.log_softmax(tag_space, dim=1)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        \n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        \n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_channel = max(map(len, processed_tokens)) #max sentence length just for english\n",
    "embedding_dim = 200 \n",
    "output_channel = max(map(len, processed_tokens)) #max sentence length just for english\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22837767, -0.5004736 ,  0.03795812,  0.2769011 ,  0.06002353,\n",
       "        0.2443759 ,  0.24667862, -0.5268596 ,  0.3109252 , -0.09721513,\n",
       "        0.08536617,  0.02022322,  0.38826668,  0.0800725 ,  0.01506799,\n",
       "        0.18854944, -0.02170865,  0.22188829, -0.01396991, -0.09093815,\n",
       "        0.07118342, -0.21627381, -0.03814943, -0.09304668, -0.08171362,\n",
       "       -0.00936724,  0.00458454,  0.09245228,  0.03126896, -0.22682981,\n",
       "        0.299948  , -0.07215133, -0.04840038, -0.30835494, -0.27917308,\n",
       "        0.17136611,  0.05805624, -0.21451256,  0.07856762, -0.12793331,\n",
       "       -0.27447063,  0.04271372,  0.12385953,  0.03228279, -0.4318502 ,\n",
       "       -0.00466134,  0.12952508,  0.12921299, -0.4957934 , -0.2114573 ,\n",
       "       -0.2827603 , -0.16331264, -0.48308343,  0.66749406, -0.4567007 ,\n",
       "        0.09083649,  0.3733607 , -0.1783615 ,  0.07900339,  0.11910054,\n",
       "       -0.18478079, -0.3049171 ,  0.18541874,  0.03618794, -0.00326595,\n",
       "        0.06624629,  0.0454898 , -0.02409362, -0.16452976,  0.02812767,\n",
       "       -0.03078479, -0.20783675, -0.16054147,  0.15303516, -0.0255521 ,\n",
       "       -0.0013181 ,  0.6520695 ,  0.06604471,  0.05321269, -0.04665343,\n",
       "       -0.12789695,  0.20566657, -0.17044908, -0.14685234, -0.01704394,\n",
       "        0.12047333,  0.14537469, -0.17362867,  0.52035034,  0.17230222,\n",
       "       -0.23933901, -0.2693685 ,  0.04188803, -0.10543806,  0.25154972,\n",
       "       -0.13223879,  0.10018335, -0.00123936, -0.09952165, -0.04815073,\n",
       "       -0.07007961, -0.06899633,  0.02319484, -0.0352538 , -0.0304815 ,\n",
       "       -0.34932822, -0.33299145,  0.19273281,  0.29741657, -0.05413064,\n",
       "       -0.2832284 , -0.3406367 ,  0.19894078, -0.16747457,  0.35970956,\n",
       "        0.3073346 , -0.31861722,  0.02498828,  0.11626031,  0.01996399,\n",
       "       -0.00478921,  0.0173524 ,  0.05792423,  0.00420092, -0.08506414,\n",
       "       -0.64927006,  0.00175374,  0.1491171 , -0.11759008, -0.13777864,\n",
       "       -0.22007838,  0.03952278,  0.07920182, -0.3339041 , -0.20209748,\n",
       "        0.2536904 ,  0.3064118 ,  0.21395127, -0.00783767, -0.04906023,\n",
       "       -0.01179575,  0.11673953, -0.00716219,  0.01952332, -0.13982141,\n",
       "        0.02539791,  0.3848009 ,  0.08534929,  0.1409253 ,  0.1142863 ,\n",
       "       -0.15729912, -0.2110354 ,  0.19420436, -0.1676652 , -0.04315299,\n",
       "        0.10405324, -0.03459496, -0.21469253,  0.00758332, -0.20575929,\n",
       "       -0.15528953,  0.20097217,  0.15408778,  0.01555694, -0.11711626,\n",
       "       -0.27376458,  0.31205243,  0.2183661 ,  0.05779367, -0.1328975 ,\n",
       "        0.07399072, -0.09814265,  0.04438574, -0.03056746, -0.00898725,\n",
       "       -0.17422172,  0.11955601,  0.03632375, -0.17525393,  0.05055578,\n",
       "       -0.02876653, -0.20269635,  0.08359186, -0.07555664, -0.0584923 ,\n",
       "       -0.11586109, -0.0228888 ,  0.09688257,  0.07393039, -0.16506368,\n",
       "       -0.00711951, -0.0094192 ,  0.0636051 , -0.26395667, -0.01573581,\n",
       "        0.21366417, -0.09114897, -0.09307356, -0.16196229, -0.08420379],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_word_vector(\"in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11999\n"
     ]
    }
   ],
   "source": [
    "training_data = utils.substitute_with_UNK(processed_tokens,1)\n",
    "print(len(training_data))\n",
    "\n",
    "word_to_ix = {}\n",
    "ix_to_word = {}\n",
    "tag_to_ix = {}\n",
    "ix_to_tag = {}\n",
    "for sent in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            ix_to_word[word_to_ix[word]] = word\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        ix_to_tag[tag_to_ix[tag]] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'well': 0,\n",
       " 'said': 1,\n",
       " 'sonu': 2,\n",
       " 'courage': 3,\n",
       " 'stand': 4,\n",
       " 'UNK': 5,\n",
       " 'muslims': 6,\n",
       " 'private': 7,\n",
       " 'banks': 8,\n",
       " 'atm': 9,\n",
       " 'like': 10,\n",
       " 'hdfc': 11,\n",
       " 'icici': 12,\n",
       " 'etc': 13,\n",
       " 'cash': 14,\n",
       " 'public': 15,\n",
       " 'sector': 16,\n",
       " 'bank': 17,\n",
       " 'working': 18,\n",
       " 'question': 19,\n",
       " 'pakistan': 20,\n",
       " 'adhere': 21,\n",
       " 'fake': 22,\n",
       " 'know': 23,\n",
       " 'meaning': 24,\n",
       " 'unity': 25,\n",
       " 'thoughts': 26,\n",
       " 'others': 27,\n",
       " 'rascals': 28,\n",
       " 'gathered': 29,\n",
       " 'r': 30,\n",
       " 'cow': 31,\n",
       " 'slaughter': 32,\n",
       " 'course': 33,\n",
       " 'stop': 34,\n",
       " 'leather': 35,\n",
       " 'manufacturing': 36,\n",
       " 'happens': 37,\n",
       " 'wondering': 38,\n",
       " 'educated': 39,\n",
       " 'ambassador': 40,\n",
       " 'struggling': 41,\n",
       " 'pay': 42,\n",
       " 'credit': 43,\n",
       " 'debit': 44,\n",
       " 'decent': 45,\n",
       " 'restaurant': 46,\n",
       " 'cant': 47,\n",
       " 'imagine': 48,\n",
       " 'diplomat': 49,\n",
       " 'developed': 50,\n",
       " 'nation': 51,\n",
       " 'card': 52,\n",
       " 'needs': 53,\n",
       " 'dinner': 54,\n",
       " 'inflation': 55,\n",
       " 'react': 56,\n",
       " 'demon': 57,\n",
       " 'good': 58,\n",
       " 'job': 59,\n",
       " 'creating': 60,\n",
       " 'problem': 61,\n",
       " 'n': 62,\n",
       " 'false': 63,\n",
       " 'news': 64,\n",
       " 'indian': 65,\n",
       " 'media': 66,\n",
       " 'simply': 67,\n",
       " 'misguiding': 68,\n",
       " 'hatred': 69,\n",
       " 'v': 70,\n",
       " 'careful': 71,\n",
       " 'spreading': 72,\n",
       " 'shame': 73,\n",
       " 'permanent': 74,\n",
       " 'friends': 75,\n",
       " 'interest': 76,\n",
       " 'deepak': 77,\n",
       " 'kumar': 78,\n",
       " 'sharma': 79,\n",
       " 'saab': 80,\n",
       " 'chalo': 81,\n",
       " 'aap': 82,\n",
       " 'ki': 83,\n",
       " 'ye': 84,\n",
       " 'baat': 85,\n",
       " 'ek': 86,\n",
       " 'baar': 87,\n",
       " 'mann': 88,\n",
       " 'li': 89,\n",
       " 'whatever': 90,\n",
       " 'pm': 91,\n",
       " 'talked': 92,\n",
       " 'kya': 93,\n",
       " 'kia': 94,\n",
       " 'us': 95,\n",
       " 'main': 96,\n",
       " 'campaigner': 97,\n",
       " 'bjp': 98,\n",
       " 'nothing': 99,\n",
       " 'else': 100,\n",
       " 'implementing': 101,\n",
       " 'bills': 102,\n",
       " 'opposed': 103,\n",
       " 'example': 104,\n",
       " 'fdi': 105,\n",
       " 'gst': 106,\n",
       " 'list': 107,\n",
       " 'long': 108,\n",
       " 'communist': 109,\n",
       " 'parties': 110,\n",
       " 'killed': 111,\n",
       " 'opponents': 112,\n",
       " 'wb': 113,\n",
       " 'years': 114,\n",
       " 'ruling': 115,\n",
       " 'guys': 116,\n",
       " 'counter': 117,\n",
       " 'modi': 118,\n",
       " 'govt': 119,\n",
       " 'decisions': 120,\n",
       " 'fact': 121,\n",
       " 'black': 122,\n",
       " 'money': 123,\n",
       " 'cleanup': 124,\n",
       " 'taken': 125,\n",
       " 'many': 126,\n",
       " 'discussions': 127,\n",
       " 'channel': 128,\n",
       " 'individuals': 129,\n",
       " 'much': 130,\n",
       " 'efforts': 131,\n",
       " 'made': 132,\n",
       " 'make': 133,\n",
       " 'people': 134,\n",
       " 'scared': 135,\n",
       " 'provoke': 136,\n",
       " 'rss': 137,\n",
       " 'time': 138,\n",
       " 'ban': 139,\n",
       " 'terrorist': 140,\n",
       " 'organization': 141,\n",
       " 'acting': 142,\n",
       " 'watch': 143,\n",
       " 'option': 144,\n",
       " 'happy': 145,\n",
       " 'diwali': 146,\n",
       " 'let': 147,\n",
       " 'wish': 148,\n",
       " 'next': 149,\n",
       " 'one': 150,\n",
       " 'year': 151,\n",
       " 'health': 152,\n",
       " 'wealth': 153,\n",
       " 'growth': 154,\n",
       " 'economy': 155,\n",
       " 'lolz': 156,\n",
       " 'gonna': 157,\n",
       " 'employ': 158,\n",
       " 'large': 159,\n",
       " 'number': 160,\n",
       " 'cyber': 161,\n",
       " 'security': 162,\n",
       " 'illiterates': 163,\n",
       " 'profile': 164,\n",
       " 'funny': 165,\n",
       " 'stupid': 166,\n",
       " 'absolutely': 167,\n",
       " 'deeper': 168,\n",
       " 'brown': 169,\n",
       " 'sahib': 170,\n",
       " 'anti': 171,\n",
       " 'national': 172,\n",
       " 'leftist': 173,\n",
       " 'commies': 174,\n",
       " 'muslim': 175,\n",
       " 'hindu': 176,\n",
       " 'sick': 177,\n",
       " 'milked': 178,\n",
       " 'secularism': 179,\n",
       " 'worth': 180,\n",
       " 'render': 181,\n",
       " 'mere': 182,\n",
       " 'failed': 183,\n",
       " 'slogan': 184,\n",
       " 'continuation': 185,\n",
       " 'british': 186,\n",
       " 'policy': 187,\n",
       " 'divide': 188,\n",
       " 'rule': 189,\n",
       " 'idea': 190,\n",
       " 'used': 191,\n",
       " 'heart': 192,\n",
       " 'hindus': 193,\n",
       " 'hinduism': 194,\n",
       " 'hindustan': 195,\n",
       " 'nifty': 196,\n",
       " 'day': 197,\n",
       " 'moving': 198,\n",
       " 'average': 199,\n",
       " 'see': 200,\n",
       " 'terrror': 201,\n",
       " 'pak': 202,\n",
       " 'afgan': 203,\n",
       " 'india': 204,\n",
       " 'development': 205,\n",
       " 'step': 206,\n",
       " 'first': 207,\n",
       " 'gaurakshak': 208,\n",
       " 'assam': 209,\n",
       " 'villagers': 210,\n",
       " 'beaten': 211,\n",
       " 'two': 212,\n",
       " 'thieves': 213,\n",
       " 'things': 214,\n",
       " 'happen': 215,\n",
       " 'try': 216,\n",
       " 'steal': 217,\n",
       " 'property': 218,\n",
       " 'farmers': 219,\n",
       " 'dont': 220,\n",
       " 'need': 221,\n",
       " 'monsters': 222,\n",
       " 'u': 223,\n",
       " 'tht': 224,\n",
       " 'hv': 225,\n",
       " 'thn': 226,\n",
       " 'flats': 227,\n",
       " 'land': 228,\n",
       " 'legal': 229,\n",
       " 'rest': 230,\n",
       " 'surrender': 231,\n",
       " 'properties': 232,\n",
       " 'upon': 233,\n",
       " 'bengal': 234,\n",
       " 'kolkata': 235,\n",
       " 'states': 236,\n",
       " 'compared': 237,\n",
       " 'bihar': 238,\n",
       " 'wake': 239,\n",
       " 'mitra': 240,\n",
       " 'use': 241,\n",
       " 'knowledge': 242,\n",
       " 'help': 243,\n",
       " 'rather': 244,\n",
       " 'lecture': 245,\n",
       " 'oh': 246,\n",
       " 'army': 247,\n",
       " 'say': 248,\n",
       " 'porki': 249,\n",
       " 'hijada': 250,\n",
       " 'whose': 251,\n",
       " 'country': 252,\n",
       " 'excepted': 253,\n",
       " 'bodies': 254,\n",
       " 'dickless': 255,\n",
       " 'coward': 256,\n",
       " 'kargil': 257,\n",
       " 'war': 258,\n",
       " 'submitted': 259,\n",
       " 'infront': 260,\n",
       " 'east': 261,\n",
       " 'became': 262,\n",
       " 'bangladesh': 263,\n",
       " 'even': 264,\n",
       " 'save': 265,\n",
       " 'school': 266,\n",
       " 'childrens': 267,\n",
       " 'merciless': 268,\n",
       " 'killings': 269,\n",
       " 'forces': 270,\n",
       " 'really': 271,\n",
       " 'reached': 272,\n",
       " 'lahore': 273,\n",
       " 'would': 274,\n",
       " 'lone': 275,\n",
       " 'boss': 276,\n",
       " 'supporters': 277,\n",
       " 'kejriwal': 278,\n",
       " 'want': 279,\n",
       " 'get': 280,\n",
       " 'rid': 281,\n",
       " 'indians': 282,\n",
       " 'hear': 283,\n",
       " 'loud': 284,\n",
       " 'cries': 285,\n",
       " 'jio': 286,\n",
       " 'customers': 287,\n",
       " 'converted': 288,\n",
       " 'prime': 289,\n",
       " 'extended': 290,\n",
       " 'date': 291,\n",
       " 'loot': 292,\n",
       " 'mandate': 293,\n",
       " 'democracy': 294,\n",
       " 'thing': 295,\n",
       " 'eyes': 296,\n",
       " 'judiciary': 297,\n",
       " 'accused': 298,\n",
       " 'deserved': 299,\n",
       " 'ever': 300,\n",
       " 'keep': 301,\n",
       " 'bad': 302,\n",
       " 'eye': 303,\n",
       " 'without': 304,\n",
       " 'mercy': 305,\n",
       " 'occur': 306,\n",
       " 'countries': 307,\n",
       " 'buy': 308,\n",
       " 'every': 309,\n",
       " 'decline': 310,\n",
       " 'target': 311,\n",
       " 'frame': 312,\n",
       " 'government': 313,\n",
       " 'office': 314,\n",
       " 'implement': 315,\n",
       " 'online': 316,\n",
       " 'transactions': 317,\n",
       " 'also': 318,\n",
       " 'show': 319,\n",
       " 'account': 320,\n",
       " 'transaction': 321,\n",
       " 'running': 322,\n",
       " 'elections': 323,\n",
       " 'rti': 324,\n",
       " 'anna': 325,\n",
       " 'hi': 326,\n",
       " 'lokpal': 327,\n",
       " 'bill': 328,\n",
       " 'aur': 329,\n",
       " 'gaye': 330,\n",
       " 'karne': 331,\n",
       " 'mae': 332,\n",
       " 'bhi': 333,\n",
       " 'nahi': 334,\n",
       " 'unfortunately': 335,\n",
       " 'wat': 336,\n",
       " 'capable': 337,\n",
       " 'dint': 338,\n",
       " 'vote': 339,\n",
       " 'modiji': 340,\n",
       " 'crap': 341,\n",
       " 'wait': 342,\n",
       " 'forgot': 343,\n",
       " 'lakh': 344,\n",
       " 'porkis': 345,\n",
       " 'soldier': 346,\n",
       " 'freed': 347,\n",
       " 'way': 348,\n",
       " 'channels': 349,\n",
       " 'anuj': 350,\n",
       " 'brought': 351,\n",
       " 'divis': 352,\n",
       " 'days': 353,\n",
       " 'tata': 354,\n",
       " 'motor': 355,\n",
       " 'dvr': 356,\n",
       " 'talk': 357,\n",
       " 'common': 358,\n",
       " 'man': 359,\n",
       " 'suffering': 360,\n",
       " 'thousands': 361,\n",
       " 'laid': 362,\n",
       " 'lives': 363,\n",
       " 'freedom': 364,\n",
       " 'little': 365,\n",
       " 'trouble': 366,\n",
       " 'queues': 367,\n",
       " 'greater': 368,\n",
       " 'visited': 369,\n",
       " 'cont': 370,\n",
       " 'able': 371,\n",
       " 'withdraw': 372,\n",
       " 'nagaland': 373,\n",
       " 'hate': 374,\n",
       " 'truth': 375,\n",
       " 'lok': 376,\n",
       " 'sabha': 377,\n",
       " 'seats': 378,\n",
       " 'arunachal': 379,\n",
       " 'yes': 380,\n",
       " 'matters': 381,\n",
       " 'becoming': 382,\n",
       " 'including': 383,\n",
       " 'manipur': 384,\n",
       " 'tripura': 385,\n",
       " 'ji': 386,\n",
       " 'pain': 387,\n",
       " 'real': 388,\n",
       " 'gain': 389,\n",
       " 'hell': 390,\n",
       " 'political': 391,\n",
       " 'corrupt': 392,\n",
       " 'anyone': 393,\n",
       " 'may': 394,\n",
       " 'supporting': 395,\n",
       " 'naxalites': 396,\n",
       " 'kill': 397,\n",
       " 'soldiers': 398,\n",
       " 'taking': 399,\n",
       " 'side': 400,\n",
       " 'jnu': 401,\n",
       " 'students': 402,\n",
       " 'shout': 403,\n",
       " 'bharat': 404,\n",
       " 'ke': 405,\n",
       " 'tukde': 406,\n",
       " 'guru': 407,\n",
       " 'hai': 408,\n",
       " 'recent': 409,\n",
       " 'wrong': 410,\n",
       " 'chanting': 411,\n",
       " 'attributed': 412,\n",
       " 'hence': 413,\n",
       " 'antinational': 414,\n",
       " 'poor': 415,\n",
       " 'girl': 416,\n",
       " 'instead': 417,\n",
       " 'helping': 418,\n",
       " 'jump': 419,\n",
       " 'conclusions': 420,\n",
       " 'impulsive': 421,\n",
       " 'saying': 422,\n",
       " 'screw': 423,\n",
       " 'bajaj': 424,\n",
       " 'making': 425,\n",
       " 'udhav': 426,\n",
       " 'shown': 427,\n",
       " 'place': 428,\n",
       " 'bravo': 429,\n",
       " 'shivsena': 430,\n",
       " 'back': 431,\n",
       " 'wrongly': 432,\n",
       " 'claiming': 433,\n",
       " 'surgical': 434,\n",
       " 'strikes': 435,\n",
       " 'strike': 436,\n",
       " 'please': 437,\n",
       " 'advice': 438,\n",
       " 'npa': 439,\n",
       " 'benefit': 440,\n",
       " 'psu': 441,\n",
       " 'plz': 442,\n",
       " 'share': 443,\n",
       " 'ur': 444,\n",
       " 'view': 445,\n",
       " 'life': 446,\n",
       " 'bought': 447,\n",
       " 'offensive': 448,\n",
       " 'defensive': 449,\n",
       " 'strategy': 450,\n",
       " 'learn': 451,\n",
       " 'lessons': 452,\n",
       " 'israel': 453,\n",
       " 'give': 454,\n",
       " 'free': 455,\n",
       " 'hand': 456,\n",
       " 'covert': 457,\n",
       " 'inside': 458,\n",
       " 'bleed': 459,\n",
       " 'tollywood': 460,\n",
       " 'parbhas': 461,\n",
       " 'die': 462,\n",
       " 'fans': 463,\n",
       " 'bahubali': 464,\n",
       " 'movie': 465,\n",
       " 'compare': 466,\n",
       " 'cinema': 467,\n",
       " 'bollywood': 468,\n",
       " 'movies': 469,\n",
       " 'khan': 470,\n",
       " 'aamir': 471,\n",
       " 'salman': 472,\n",
       " 'shahrukh': 473,\n",
       " 'called': 474,\n",
       " 'civilised': 475,\n",
       " 'claim': 476,\n",
       " 'pakistani': 477,\n",
       " 'professional': 478,\n",
       " 'true': 479,\n",
       " 'professionals': 480,\n",
       " 'terrorists': 481,\n",
       " 'certainly': 482,\n",
       " 'global': 483,\n",
       " 'world': 484,\n",
       " 'understanding': 485,\n",
       " 'must': 486,\n",
       " 'prevent': 487,\n",
       " 'cross': 488,\n",
       " 'loc': 489,\n",
       " 'coming': 490,\n",
       " 'cost': 491,\n",
       " 'purpose': 492,\n",
       " 'bullets': 493,\n",
       " 'slap': 494,\n",
       " 'trade': 495,\n",
       " 'urge': 496,\n",
       " 'convince': 497,\n",
       " 'powerful': 498,\n",
       " 'nations': 499,\n",
       " 'introduce': 500,\n",
       " 'economic': 501,\n",
       " 'sanctions': 502,\n",
       " 'period': 503,\n",
       " 'state': 504,\n",
       " 'knees': 505,\n",
       " 'haram': 506,\n",
       " 'pig': 507,\n",
       " 'scavengers': 508,\n",
       " 'eat': 509,\n",
       " 'dead': 510,\n",
       " 'harm': 511,\n",
       " 'mutilation': 512,\n",
       " 'reliance': 513,\n",
       " 'giving': 514,\n",
       " 'k': 515,\n",
       " 'months': 516,\n",
       " 'subscription': 517,\n",
       " 'unlimited': 518,\n",
       " 'calling': 519,\n",
       " 'std': 520,\n",
       " 'roaming': 521,\n",
       " 'g': 522,\n",
       " 'means': 523,\n",
       " 'actual': 524,\n",
       " 'speed': 525,\n",
       " 'plan': 526,\n",
       " 'early': 527,\n",
       " 'airtel': 528,\n",
       " 'got': 529,\n",
       " 'lower': 530,\n",
       " 'rates': 531,\n",
       " 'gb': 532,\n",
       " 'internet': 533,\n",
       " 'mistry': 534,\n",
       " 'substantiate': 535,\n",
       " 'sons': 536,\n",
       " 'counsel': 537,\n",
       " 'suzlon': 538,\n",
       " 'energy': 539,\n",
       " 'purchase': 540,\n",
       " 'canadian': 541,\n",
       " 'solar': 542,\n",
       " 'baba': 543,\n",
       " 'always': 544,\n",
       " 'wants': 545,\n",
       " 'encash': 546,\n",
       " 'atmosphere': 547,\n",
       " 'hes': 548,\n",
       " 'lost': 549,\n",
       " 'dramatically': 550,\n",
       " 'km': 551,\n",
       " 'lose': 552,\n",
       " 'series': 553,\n",
       " 'shameful': 554,\n",
       " 'australian': 555,\n",
       " 'team': 556,\n",
       " 'home': 557,\n",
       " 'become': 558,\n",
       " 'leading': 559,\n",
       " 'mobile': 560,\n",
       " 'data': 561,\n",
       " 'usage': 562,\n",
       " 'mukesh': 563,\n",
       " 'ambani': 564,\n",
       " 'agreed': 565,\n",
       " 'intolerance': 566,\n",
       " 'groups': 567,\n",
       " 'amir': 568,\n",
       " 'talking': 569,\n",
       " 'thinks': 570,\n",
       " 'tweet': 571,\n",
       " 'express': 572,\n",
       " 'found': 573,\n",
       " 'shirish': 574,\n",
       " 'kunder': 575,\n",
       " 'personality': 576,\n",
       " 'front': 577,\n",
       " 'page': 578,\n",
       " 'standard': 579,\n",
       " 'gone': 580,\n",
       " 'wife': 581,\n",
       " 'sitaram': 582,\n",
       " 'resident': 583,\n",
       " 'editor': 584,\n",
       " 'ok': 585,\n",
       " 'entertainment': 586,\n",
       " 'karan': 587,\n",
       " 'johar': 588,\n",
       " 'might': 589,\n",
       " 'justin': 590,\n",
       " 'bieber': 591,\n",
       " 'kick': 592,\n",
       " 'sixth': 593,\n",
       " 'season': 594,\n",
       " 'never': 595,\n",
       " 'win': 596,\n",
       " 'mohit': 597,\n",
       " 'soni': 598,\n",
       " 'reality': 599,\n",
       " 'bro': 600,\n",
       " 'read': 601,\n",
       " 'carefully': 602,\n",
       " 'unbiased': 603,\n",
       " 'approach': 604,\n",
       " 'regardless': 605,\n",
       " 'medal': 606,\n",
       " 'proud': 607,\n",
       " 'pakisthan': 608,\n",
       " 'guts': 609,\n",
       " 'admit': 610,\n",
       " 'liars': 611,\n",
       " 'fan': 612,\n",
       " 'mate': 613,\n",
       " 'think': 614,\n",
       " 'zee': 615,\n",
       " 'nano': 616,\n",
       " 'chit': 617,\n",
       " 'notes': 618,\n",
       " 'lets': 619,\n",
       " 'intellect': 620,\n",
       " 'coz': 621,\n",
       " 'low': 622,\n",
       " 'judge': 623,\n",
       " 'based': 624,\n",
       " 'choices': 625,\n",
       " 'majority': 626,\n",
       " 'kashmiris': 627,\n",
       " 'pro': 628,\n",
       " 'unable': 629,\n",
       " 'effectively': 630,\n",
       " 'order': 631,\n",
       " 'bring': 632,\n",
       " 'normalcy': 633,\n",
       " 'kashmir': 634,\n",
       " 'invitation': 635,\n",
       " 'going': 636,\n",
       " 'movement': 637,\n",
       " 'solve': 638,\n",
       " 'mentioned': 639,\n",
       " 'regards': 640,\n",
       " 'email': 641,\n",
       " 'hemraj_jain': 642,\n",
       " 'yahoo': 643,\n",
       " 'com': 644,\n",
       " 'lynching': 645,\n",
       " 'part': 646,\n",
       " 'struggle': 647,\n",
       " 'chaddis': 648,\n",
       " 'chamchas': 649,\n",
       " 'sham': 650,\n",
       " 'dear': 651,\n",
       " 'playing': 652,\n",
       " 'emotions': 653,\n",
       " 'votes': 654,\n",
       " 'five': 655,\n",
       " 'worst': 656,\n",
       " 'riots': 657,\n",
       " 'post': 658,\n",
       " 'independence': 659,\n",
       " 'congress': 660,\n",
       " 'cms': 661,\n",
       " 'riot': 662,\n",
       " 'hit': 663,\n",
       " 'remember': 664,\n",
       " 'japanese': 665,\n",
       " 'last': 666,\n",
       " 'cup': 667,\n",
       " 'mizo': 668,\n",
       " 'ii': 669,\n",
       " 'famous': 670,\n",
       " 'battle': 671,\n",
       " 'britishers': 672,\n",
       " 'p': 673,\n",
       " 'intended': 674,\n",
       " 'work': 675,\n",
       " 'getting': 676,\n",
       " 'completed': 677,\n",
       " 'hero': 678,\n",
       " 'honda': 679,\n",
       " 'hazare': 680,\n",
       " 'agent': 681,\n",
       " 'ask': 682,\n",
       " 'khans': 683,\n",
       " 'ways': 684,\n",
       " 'drunk': 685,\n",
       " 'start': 686,\n",
       " 'human': 687,\n",
       " 'sure': 688,\n",
       " 'white': 689,\n",
       " 'american': 690,\n",
       " 'christian': 691,\n",
       " 'otherwise': 692,\n",
       " 'headline': 693,\n",
       " 'beer': 694,\n",
       " 'drinking': 695,\n",
       " 'word': 696,\n",
       " 'calls': 697,\n",
       " 'trump': 698,\n",
       " 'laughing': 699,\n",
       " 'stock': 700,\n",
       " 'great': 701,\n",
       " 'orange': 702,\n",
       " 'call': 703,\n",
       " 'cannot': 704,\n",
       " 'least': 705,\n",
       " 'support': 706,\n",
       " 'trying': 707,\n",
       " 'something': 708,\n",
       " 'better': 709,\n",
       " 'today': 710,\n",
       " 'whole': 711,\n",
       " 'recognise': 712,\n",
       " 'promising': 713,\n",
       " 'banning': 714,\n",
       " 'beef': 715,\n",
       " 'north': 716,\n",
       " 'hypocrisy': 717,\n",
       " 'best': 718,\n",
       " 'take': 719,\n",
       " 'guy': 720,\n",
       " 'seriously': 721,\n",
       " 'contribution': 722,\n",
       " 'producing': 723,\n",
       " 'three': 724,\n",
       " 'children': 725,\n",
       " 'waiting': 726,\n",
       " 'soon': 727,\n",
       " 'grab': 728,\n",
       " 'sympathy': 729,\n",
       " 'shows': 730,\n",
       " 'love': 731,\n",
       " 'farce': 732,\n",
       " 'happening': 733,\n",
       " 'copy': 734,\n",
       " 'pasting': 735,\n",
       " 'comment': 736,\n",
       " 'everywhere': 737,\n",
       " 'tired': 738,\n",
       " 'amma': 739,\n",
       " 'remain': 740,\n",
       " 'hearts': 741,\n",
       " 'noble': 742,\n",
       " 'soul': 743,\n",
       " 'peace': 744,\n",
       " 'per': 745,\n",
       " 'total': 746,\n",
       " 'constituency': 747,\n",
       " 'given': 748,\n",
       " 'irony': 749,\n",
       " 'display': 750,\n",
       " 'picture': 751,\n",
       " 'end': 752,\n",
       " 'seeing': 753,\n",
       " 'innocent': 754,\n",
       " 'dying': 755,\n",
       " 'praying': 756,\n",
       " 'motors': 757,\n",
       " 'reaction': 758,\n",
       " 'entered': 759,\n",
       " 'thief': 760,\n",
       " 'territory': 761,\n",
       " 'men': 762,\n",
       " 'cowards': 763,\n",
       " 'informed': 764,\n",
       " 'defeat': 765,\n",
       " 'bihari': 766,\n",
       " 'elect': 767,\n",
       " 'jdu': 768,\n",
       " 'burden': 769,\n",
       " 'porinju': 770,\n",
       " 'markets': 771,\n",
       " 'new': 772,\n",
       " 'high': 773,\n",
       " 'budget': 774,\n",
       " 'seculars': 775,\n",
       " 'ruled': 776,\n",
       " 'yrs': 777,\n",
       " 'banned': 778,\n",
       " 'lol': 779,\n",
       " 'salute': 780,\n",
       " 'friend': 781,\n",
       " 'puppet': 782,\n",
       " 'came': 783,\n",
       " 'mcd': 784,\n",
       " 'election': 785,\n",
       " 'chutiya': 786,\n",
       " 'benefits': 787,\n",
       " 'areas': 788,\n",
       " 'ing': 789,\n",
       " 'roads': 790,\n",
       " 'weak': 791,\n",
       " 'dis': 792,\n",
       " 'university': 793,\n",
       " 'gaurav': 794,\n",
       " 'seth': 795,\n",
       " 'mishra': 796,\n",
       " 'shubham': 797,\n",
       " 'singh': 798,\n",
       " 'alright': 799,\n",
       " 'match': 800,\n",
       " 'losing': 801,\n",
       " 'winning': 802,\n",
       " 'game': 803,\n",
       " 'fired': 804,\n",
       " 'across': 805,\n",
       " 'border': 806,\n",
       " 'went': 807,\n",
       " 'mutilate': 808,\n",
       " 'corpses': 809,\n",
       " 'sorry': 810,\n",
       " 'hitting': 811,\n",
       " 'highs': 812,\n",
       " 'within': 813,\n",
       " 'expiry': 814,\n",
       " 'sir': 815,\n",
       " 'president': 816,\n",
       " 'cm': 817,\n",
       " 'mp': 818,\n",
       " 'mla': 819,\n",
       " 'road': 820,\n",
       " 'food': 821,\n",
       " 'plastic': 822,\n",
       " 'accept': 823,\n",
       " 'mini': 824,\n",
       " 'elements': 825,\n",
       " 'exists': 826,\n",
       " 'funded': 827,\n",
       " 'enemy': 828,\n",
       " 'weaken': 829,\n",
       " 'integrity': 830,\n",
       " 'traitors': 831,\n",
       " 'politics': 832,\n",
       " 'hyderabad': 833,\n",
       " 'section': 834,\n",
       " 'fringe': 835,\n",
       " 'short': 836,\n",
       " 'term': 837,\n",
       " 'monetary': 838,\n",
       " 'gains': 839,\n",
       " 'prefer': 840,\n",
       " 'destabilize': 841,\n",
       " 'following': 842,\n",
       " 'external': 843,\n",
       " 'resolving': 844,\n",
       " 'issue': 845,\n",
       " 'china': 846,\n",
       " 'distant': 847,\n",
       " 'dream': 848,\n",
       " 'mr': 849,\n",
       " 'unless': 850,\n",
       " 'rotten': 851,\n",
       " 'fishes': 852,\n",
       " 'pond': 853,\n",
       " 'expect': 854,\n",
       " 'clean': 855,\n",
       " 'system': 856,\n",
       " 'important': 857,\n",
       " 'living': 858,\n",
       " 'upto': 859,\n",
       " 'expectations': 860,\n",
       " 'citizens': 861,\n",
       " 'blindly': 862,\n",
       " 'religion': 863,\n",
       " 'frankly': 864,\n",
       " 'speaking': 865,\n",
       " 'activists': 866,\n",
       " 'minded': 867,\n",
       " 'experience': 868,\n",
       " 'group': 869,\n",
       " 'activist': 870,\n",
       " 'check': 871,\n",
       " 'politicians': 872,\n",
       " 'beurocrats': 873,\n",
       " 'functioning': 874,\n",
       " 'properly': 875,\n",
       " 'guess': 876,\n",
       " 'daily': 877,\n",
       " 'right': 878,\n",
       " 'congratulations': 879,\n",
       " 'prevails': 880,\n",
       " 'delivers': 881,\n",
       " 'vested': 882,\n",
       " 'often': 883,\n",
       " 'search': 884,\n",
       " 'aggressive': 885,\n",
       " 'activism': 886,\n",
       " 'welspun': 887,\n",
       " 'rec': 888,\n",
       " 'wockhardt': 889,\n",
       " 'gati': 890,\n",
       " 'metalyst': 891,\n",
       " 'bear': 892,\n",
       " 'befitting': 893,\n",
       " 'reply': 894,\n",
       " 'however': 895,\n",
       " 'reservation': 896,\n",
       " 'poverty': 897,\n",
       " 'alleviation': 898,\n",
       " 'scheme': 899,\n",
       " 'social': 900,\n",
       " 'historically': 901,\n",
       " 'exploited': 902,\n",
       " 'marginalized': 903,\n",
       " 'rendered': 904,\n",
       " 'weaker': 905,\n",
       " 'wanted': 906,\n",
       " 'window': 907,\n",
       " 'obcs': 908,\n",
       " 'supreme': 909,\n",
       " 'court': 910,\n",
       " 'says': 911,\n",
       " 'create': 912,\n",
       " 'subwindow': 913,\n",
       " 'breaching': 914,\n",
       " 'barrier': 915,\n",
       " 'basis': 916,\n",
       " 'jokes': 917,\n",
       " 'business': 918,\n",
       " 'fear': 919,\n",
       " 'uncertainty': 920,\n",
       " 'speech': 921,\n",
       " 'another': 922,\n",
       " 'young': 923,\n",
       " 'person': 924,\n",
       " 'image': 925,\n",
       " 'btwn': 926,\n",
       " 'blame': 927,\n",
       " 'soaps': 928,\n",
       " 'women': 929,\n",
       " 'mental': 930,\n",
       " 'violence': 931,\n",
       " 'centre': 932,\n",
       " 'importantly': 933,\n",
       " 'nominated': 934,\n",
       " 'payed': 935,\n",
       " 'police': 936,\n",
       " 'kept': 937,\n",
       " 'promised': 938,\n",
       " 'rbi': 939,\n",
       " 'antonio': 940,\n",
       " 'god': 941,\n",
       " 'everything': 942,\n",
       " 'everyone': 943,\n",
       " 'followers': 944,\n",
       " 'dharma': 945,\n",
       " 'history': 946,\n",
       " 'kills': 947,\n",
       " 'thousand': 948,\n",
       " 'gujrat': 949,\n",
       " 'open': 950,\n",
       " 'mouth': 951,\n",
       " 'credibility': 952,\n",
       " 'conveniently': 953,\n",
       " 'forgotten': 954,\n",
       " 'saffron': 955,\n",
       " 'skin': 956,\n",
       " 'transformed': 957,\n",
       " 'face': 958,\n",
       " 'major': 959,\n",
       " 'quietly': 960,\n",
       " 'adopted': 961,\n",
       " 'stood': 962,\n",
       " 'loudspeakers': 963,\n",
       " 'azaan': 964,\n",
       " 'non': 965,\n",
       " 'sleep': 966,\n",
       " 'peacefully': 967,\n",
       " 'verdict': 968,\n",
       " 'fire': 969,\n",
       " 'civilians': 970,\n",
       " 'mutilating': 971,\n",
       " 'fully': 972,\n",
       " 'justified': 973,\n",
       " 'mohammad': 974,\n",
       " 'partition': 975,\n",
       " 'theory': 976,\n",
       " 'wanna': 977,\n",
       " 'meet': 978,\n",
       " 'iphone': 979,\n",
       " 'user': 980,\n",
       " 'friendly': 981,\n",
       " 'big': 982,\n",
       " 'decision': 983,\n",
       " 'clear': 984,\n",
       " 'air': 985,\n",
       " 'hello': 986,\n",
       " 'views': 987,\n",
       " 'dial': 988,\n",
       " 'rally': 989,\n",
       " 'continue': 990,\n",
       " 'profit': 991,\n",
       " 'booking': 992,\n",
       " 'film': 993,\n",
       " 'phillauri': 994,\n",
       " 'depend': 995,\n",
       " 'humanity': 996,\n",
       " 'hope': 997,\n",
       " 'wishes': 998,\n",
       " 'point': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for LSTM\n",
    "hidden_dim = 128\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear)\n",
    "loss_function = nn.NLLLoss()\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "target = torch.tensor([1])\n",
    "output = model(input1)\n",
    "loss = loss_function(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0260, -0.6090, -0.9867,  ...,  1.9890,  2.0411, -0.1198],\n",
       "         [-1.0022, -1.0795,  0.5597,  ...,  1.2189, -2.1863,  0.2827],\n",
       "         [ 0.3739, -0.5493,  2.1233,  ...,  0.2440,  0.2052,  0.0521],\n",
       "         ...,\n",
       "         [ 0.6488,  0.2645, -0.7247,  ...,  0.4141,  0.7687, -0.7324],\n",
       "         [-0.3365,  0.4911, -1.5591,  ...,  0.5042, -0.2796,  0.3819],\n",
       "         [ 1.1874, -0.5796, -1.1246,  ...,  1.0161, -1.7749, -0.5526]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(batch_size,input_channel,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
