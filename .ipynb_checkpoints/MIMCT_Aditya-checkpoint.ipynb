{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"english/agr_en_train.csv\"\n",
    "\n",
    "# preprocessing english tweets.\n",
    "#ingesting english csv file\n",
    "df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "\n",
    "df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "\n",
    "comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "tags = np.asarray(df['annotation'])\n",
    "print((len(comments)))\n",
    "print(len(tags))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "processed_tokens = []\n",
    "for comment in comments:\n",
    "#    comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "    comment = comment.lower()   #lower casing each tweets\n",
    "    Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "    URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "    new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "    sentence = []\n",
    "    for word in new_words:\n",
    "        if word not in stop_words:           #checking for stop words on each sentence\n",
    "            sentence.append(word)\n",
    "    processed_tokens.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "\n",
    "Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "hindi_tags = np.asarray(df1['annotation'])\n",
    "print((hindi_comments[1])) \n",
    "processed_Hindi_tokens = []\n",
    "for comment in hindi_comments:\n",
    "#    comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "    comment = comment.lower()   #lower casing each tweets\n",
    "    Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "    URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "    Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "    if (isEnglish(Emoji_removal) == True):\n",
    "        Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "    processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "print(processed_Hindi_tokens[0])\n",
    "print(processed_Hindi_tokens[11])\n",
    "print(processed_Hindi_tokens[6])\n",
    "print(processed_Hindi_tokens[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "print(t_dict)\n",
    "t_dict = np.asarray(t_dict)\n",
    "print(\"After NP array\")\n",
    "print(t_dict)\n",
    "\n",
    "#--------------profanity dictionary\n",
    "profanity_dict = \"ProfanityText.txt\"\n",
    "P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "P_dict['English'] = P_dict['English'].str.strip()\n",
    "print(\"PRofanity\")\n",
    "print(P_dict)\n",
    "P_dict = np.asarray(P_dict)\n",
    "print(\"After NP array\")\n",
    "print(P_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "\n",
    "Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "H_hindi = np.asarray(H_dict['Hindi'])\n",
    "H_english = np.asarray(H_dict['English'])\n",
    "\n",
    "HE_dict = dict(zip(H_hindi,H_english))\n",
    "H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "EH_dict_F = {v:k for k, v in H_dict_F.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "for i in range(0,len(processed_Hindi_tokens)):\n",
    "    #print(i)\n",
    "    for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "        Str = processed_Hindi_tokens[i][j]\n",
    "        if(Str in HE_dict):\n",
    "            processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "        elif(Str in H_dict_F):\n",
    "            processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "\n",
    "print(processed_Hindi_tokens[4])\n",
    "print(processed_Hindi_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(processed_Hindi_tokens)):\n",
    "    if i == 50:\n",
    "        break\n",
    "    for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "        flag = 0\n",
    "        Str1 = (processed_Hindi_tokens[i][j])\n",
    "        max_ratio = 60\n",
    "        max_ratio_P = 75   #needs to be adjusted\n",
    "        if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "            continue\n",
    "        for l in range(0,len(P_dict)):\n",
    "            Str2 = P_dict[l][0]\n",
    "            Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "            if (Ratiostr1 >= max_ratio_P):\n",
    "                print(Ratiostr1)\n",
    "                max_ratio_P = Ratiostr1\n",
    "                flag = 1\n",
    "                print({processed_Hindi_tokens[i][j]})\n",
    "                processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                break\n",
    "        for p in EH_dict_F:\n",
    "            Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "            if(Ratiostr1 >= 98):\n",
    "                flag = 1\n",
    "                break\n",
    "        if (flag == 1):\n",
    "            continue\n",
    "        else:\n",
    "            for k in range(0,len(t_dict)):\n",
    "                Str2 = t_dict[k][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 > max_ratio):\n",
    "                    max_ratio = Ratiostr1\n",
    "                    processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "print(processed_Hindi_tokens[0])\n",
    "print(processed_Hindi_tokens[1])\n",
    "print(processed_tokens[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p = 0.20)\n",
    "        max_pooling1d = nn.MaxPool1d(kernel_size, stride = 1)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self,x):\n",
    "        cnn_output = self.CNN_Layers(x)\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        lstm_output = F.log_softmax(tag_space, dim=1)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        print(lstm_output)\n",
    "        print(cnn_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for LSTM\n",
    "hidden_dim = 64\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "model = MIMCT(input_channel, output_channel, embedding_dim, kernel_size, feature_linear)\n",
    "\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "output = model(input1)\n",
    "#create the loss cretirion and training loops\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "output = model(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "m = nn.Conv1d(1, 2,1,stride=2)\n",
    "input1 = torch.randn(10)\n",
    "cnn1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n",
    "input1 = input1.unsqueeze(0).unsqueeze(0)\n",
    "input1.shape\n",
    "cnn1d_2(input1)\n",
    "\n",
    "\n",
    "\n",
    "#------------CNN_Class----------------#\n",
    "kernel_size = [4,3,2]\n",
    "embedding_dim = 10\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "\n",
    "Layer1 = nn.Conv1d(16, 16, 4, stride=1)\n",
    "Layer2 = nn.Conv1d(16, 16, 3, stride=1)\n",
    "Layer3 = nn.Conv1d(16, 16, 2, stride=1)\n",
    "Dropout_layer = nn.Dropout(p=0.25)\n",
    "dense_layer = nn.Linear(Feature_layer3, 3)\n",
    "nn.Flatten\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 10\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "output = Layer1(input1)\n",
    "output1 = Layer2(output)\n",
    "output2 = Layer3(output1)\n",
    "output3 = Dropout_layer(output2)\n",
    "output4 = dense_layer(output3)\n",
    "nn.flatten\n",
    "\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],[0.0,  1.0],[0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)\n",
    "m.weight\n",
    "\n",
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.CNN_Layers(x)\n",
    "        return x\n",
    "\n",
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,kernel_size,feature_linear)\n",
    "\n",
    "output = model(input1)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "m = nn.Sequential( nn.Conv1d(input_channel,16,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "test_input = input1\n",
    "output = m(test_input)\n",
    "nn.flatten\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
