{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from  nltk import word_tokenize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_tokens_for_english_tweets():\n",
    "    f = \"english/agr_en_train.csv\"\n",
    "    # preprocessing english tweets.\n",
    "    #ingesting english csv file\n",
    "    df = pd.read_csv(f,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df['comment'] = df.comment.str.strip()   # removing spaces\n",
    "    comments = np.asarray(df['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    tags = np.asarray(df['annotation'])\n",
    "    print((len(comments)))\n",
    "    print(len(tags))\n",
    "    stop_words = set(stopwords.words('english'))  #english stop words list\n",
    "    processed_tokens = []\n",
    "    for comment in comments:\n",
    "    # comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")   # removal of punctuation and tokenizing\n",
    "        new_words = tokenizer.tokenize(URL_REMOVAL)\n",
    "        sentence = []\n",
    "        for word in new_words:\n",
    "            if word not in stop_words:           #checking for stop words on each sentence\n",
    "                sentence.append(word)\n",
    "        processed_tokens.append(sentence)\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-----------------For hinglish dataset\n",
    "\n",
    "def get_processed_hindi_tokens():\n",
    "    Hindi_text  = \"hindi/agr_hi_dev.csv\"\n",
    "    df1 = pd.read_csv(Hindi_text,names = ['source','comment','annotation'],encoding='UTF-8')\n",
    "    df1['comment'] = df1.comment.str.strip()   # removing spaces\n",
    "    hindi_comments = np.asarray(df1['comment'])    # dividing the dataframe into comments and tags and converting to array\n",
    "    hindi_tags = np.asarray(df1['annotation'])\n",
    "    print((hindi_comments[1])) \n",
    "    processed_Hindi_tokens = []\n",
    "    for comment in hindi_comments:\n",
    "    #   comment = \"Also see ....hw ur RSS activist caught in Burkha .... throwing beef in d holy temples...https://www.google.co.in/amp/www.india.com/news/india/burkha-clad-rss-activist-caught-throwing-beef-at-temple-pictures-go-viral-on-facebook-593154/amp/,NAGfacebook_corpus_msr_403402,On the death of 2 jawans in LOC CROSS FIRING\"\n",
    "        comment = comment.lower()   #lower casing each tweets\n",
    "        Digit_REMOVAL = re.sub(r'[0-9]+', '',comment) #removal of numbers \n",
    "        URL_REMOVAL = re.sub(r\"http\\S+\", \"\", Digit_REMOVAL) # removal of URLS\n",
    "        Emoji_removal = remove_emoji(URL_REMOVAL)\n",
    "        if (isEnglish(Emoji_removal) == True):\n",
    "            Emoji_removal = re.sub(r'[^\\w\\s]','',Emoji_removal)# removal of punctuation and tokenizing\n",
    "        processed_Hindi_tokens.append(word_tokenize(Emoji_removal))\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Transliteration and translation\n",
    "def get_transliteration_Hinglish_Hindi_dict():\n",
    "    transliteration_dict = \"transliterations.hi-en.csv\"\n",
    "    t_dict = pd.read_csv(transliteration_dict,names = ['Hinglish','Hindi'],encoding='UTF-8',sep='\\t')\n",
    "    t_dict['Hinglish'] = t_dict['Hinglish'].str.strip()\n",
    "    t_dict['Hindi'] = t_dict['Hindi'].str.strip()\n",
    "    print(t_dict)\n",
    "    t_dict = np.asarray(t_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(t_dict)\n",
    "    return t_dict\n",
    "\n",
    "#--------------profanity dictionary\n",
    "def get_profanity_dict():\n",
    "    profanity_dict = \"ProfanityText.txt\"\n",
    "    P_dict = pd.read_csv(profanity_dict,names = ['Hinglish','English'],encoding='UTF-8',sep='\\t')\n",
    "    P_dict['Hinglish'] = P_dict['Hinglish'].str.strip()\n",
    "    P_dict['English'] = P_dict['English'].str.strip()\n",
    "    print(\"Profanity\")\n",
    "    print(P_dict)\n",
    "    P_dict = np.asarray(P_dict)\n",
    "    print(\"After NP array\")\n",
    "    print(P_dict)\n",
    "    return P_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Translation of hindi text back to english-------\n",
    "def translate_hindi_to_english():\n",
    "    Hindi_dict = \"Hindi_English_Dict.csv\"\n",
    "    H_dict = pd.read_csv(Hindi_dict,names = ['Hindi','English'],encoding='UTF-8')\n",
    "\n",
    "    HE_dict_F = \"HE_dictionary_functions.csv\"\n",
    "    H_dict_F = pd.read_csv(HE_dict_F,names = ['Hindi','English'],encoding='UTF-8')\n",
    "    H_dict_F['Hindi'] = H_dict_F['Hindi'].str.strip()\n",
    "    H_dict_F['English'] = H_dict_F['English'].str.strip()\n",
    "\n",
    "    H_hindi_F = np.asarray(H_dict_F['Hindi'])\n",
    "    H_english_F = np.asarray(H_dict_F['English'])\n",
    "\n",
    "    H_dict['Hindi'] = H_dict['Hindi'].str.strip()\n",
    "    H_dict['English'] = H_dict['English'].str.strip()\n",
    "\n",
    "    H_hindi = np.asarray(H_dict['Hindi'])\n",
    "    H_english = np.asarray(H_dict['English'])\n",
    "    \n",
    "    HE_dict = dict(zip(H_hindi,H_english))\n",
    "    H_dict_F = dict(zip(H_hindi_F,H_english_F))\n",
    "\n",
    "    EH_dict = {v:k for k, v in HE_dict.items()}\n",
    "    EH_dict_F = {v:k for k, v in H_dict_F.items()}\n",
    "    \n",
    "    return HE_dict, H_dict_F, EH_dict, EH_dict_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "def get_translated_hindi_english(HE_dict, H_dict_F):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        #print(i)\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            Str = processed_Hindi_tokens[i][j]\n",
    "            if(Str in HE_dict):\n",
    "                processed_Hindi_tokens[i][j] = HE_dict[Str]\n",
    "            elif(Str in H_dict_F):\n",
    "                processed_Hindi_tokens[i][j] = H_dict_F[Str]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict):\n",
    "    for i in range(0,len(processed_Hindi_tokens)):\n",
    "        if i == 50:\n",
    "            break\n",
    "        for j in range (0,len(processed_Hindi_tokens[i])):\n",
    "            flag = 0\n",
    "            Str1 = (processed_Hindi_tokens[i][j])\n",
    "            max_ratio = 60\n",
    "            max_ratio_P = 75   #needs to be adjusted\n",
    "            if (Str1 in EH_dict): # check whether the values exists in english dictionary or not.\n",
    "                continue\n",
    "            for l in range(0,len(P_dict)):\n",
    "                Str2 = P_dict[l][0]\n",
    "                Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                if (Ratiostr1 >= max_ratio_P):\n",
    "                    print(Ratiostr1)\n",
    "                    max_ratio_P = Ratiostr1\n",
    "                    flag = 1\n",
    "                    print(processed_Hindi_tokens[i][j])\n",
    "                    processed_Hindi_tokens[i][j] = P_dict[l][1]\n",
    "                    print(f\"{flag}-{processed_Hindi_tokens[i][j]}\") \n",
    "                    break\n",
    "            for p in EH_dict_F:\n",
    "                Ratiostr1 = fuzz.ratio(Str1,str(p))\n",
    "                if(Ratiostr1 >= 98):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if (flag == 1):\n",
    "                continue\n",
    "            else:\n",
    "                for k in range(0,len(t_dict)):\n",
    "                    Str2 = t_dict[k][0]\n",
    "                    Ratiostr1 = fuzz.ratio(Str1,Str2)\n",
    "                    if (Ratiostr1 > max_ratio):\n",
    "                        max_ratio = Ratiostr1\n",
    "                        processed_Hindi_tokens[i][j] = t_dict[k][1]\n",
    "    print(processed_Hindi_tokens[0])\n",
    "    print(processed_Hindi_tokens[1])\n",
    "    print(processed_tokens[12])\n",
    "    return processed_Hindi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['I', 'I', 'I', 'we', 'we', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'you', 'that', 'those', 'those', 'they', 'they', 'this', 'it', 'she', 'she', 'it', 'she', 'we', 'you', 'they', 'they', 'we', 'they', 'they', 'I', 'we', 'we', nan, 'you', 'you', 'she', 'you', 'she', 'she', 'they', 'she', 'they', 'it', 'they', 'they', 'they', 'I', 'we', 'it', 'I', 'one', 'she', 'one', 'it', nan, 'these', 'somebody', 'anybody', 'both', 'no one', 'no one', 'someone', 'someone', 'someone', 'somebody', nan, 'no one', 'someone else', 'a few', 'something', 'anything', 'enough', nan, nan, 'everyone', 'evrything', 'the few', 'a few', 'many', 'little', 'little', 'a little', 'a lot', 'many', 'many', 'many', 'plenty', 'plenty', 'much', 'one', 'each other', 'each other', 'one another', 'one another', 'one another', 'each other', 'who', 'what', 'whose', 'who', 'whose', 'who', 'whom', 'whom', 'who', 'which', nan, 'if', 'who', 'what', 'how', 'how many', 'hello', 'where', 'where', 'how', 'how much', 'and', 'then', 'and', 'and', 'too', 'and', 'after', 'where', 'whenever', 'until', 'as long as', 'whenever', 'as soon as', nan, 'whencesoever', 'whencever', 'wherever', 'or', 'or', 'even if', 'or', 'not', 'otherwise', 'that', 'alias', 'but', 'but', 'but', 'but', 'but', 'but', 'but rather', 'otherwise', 'otherwise', 'otherwise', 'because', 'because', 'because', 'so', 'because', 'which', 'so', 'because', 'because', 'so', 'although', nan, 'although', 'nevertheless', 'nevertheless', 'nevertheless', 'because', 'that', 'that', 'that', 'ah', 'oh when', 'what', 'huh', 'oho', 'aha', 'yes', 'ah', 'really', 'hey', 'alas', 'oh', 'wonderful', 'oh', 'oh my god', 'splendid', 'ah', 'welldone', 'blessing', 'blessing', 'very good', 'excelsior', 'very good', 'excelsior', 'oh', 'ah', 'alas', 'alas', 'alas', 'alas', 'alas', 'alas', 'ouch', 'ouch', 'alas', 'alas', 'okay', 'sure', 'ah well', 'ah', 'yes', 'fine', 'boo', 'ugh', 'reproof', 'out of the way', 'out of the way', 'stop it', 'stop it', 'out of the way', nan, 'ugh', 'all right', 'all right', 'well', 'sure', 'yes', 'sure', 'indeed', 'very well', 'hurrh', 'well done', 'be happy', 'blessing', 'good luck', 'BLESS', 'BLESS', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'hello', 'enough', 'watch out', 'look out', 'adiev', 'okay', 'a hoy', 'a hoy', 'alas', 'amen', 'okay', 'really', 'stop', 'stop', 'go ahead', 'go away', 'by God', 'i swear', 'blees you', 'ahchoo', 'ahem', 'ahoy', nan, 'in between', 'before', 'according to', 'to right across', nan, 'after', 'after', 'after', 'after', 'after', 'after', 'side', 'near to', 'contrary to', 'against', 'for', 'against', 'in place of', 'against', 'by', 'against', 'ahead from', 'before', 'before', 'after', 'after', 'apporoxmately', nan, 'percent', 'near by', 'near by', 'close to', nan, nan, 'out of', 'beyond', 'far from', 'under', 'infront of', 'within', 'in', 'inplace of', 'side', 'towards', 'across', 'near by', 'by', 'by', 'by the dinct of', 'by', 'by', 'by', 'due to', 'due to', 'due to', 'for the sake of', 'reason for', 'for', 'for the sake of', 'for the sake of', 'compare to', 'compare to', 'about', 'for', 'because of', 'depending on', 'on account of', 'similar to', 'equal to', 'equal to', 'suitable to', 'suitable to', 'according to', 'like', 'like', 'like', 'like', 'like', 'in comparision to', 'like', 'like', 'like', 'according to', 'in imitation of', 'like', 'like', 'like', 'according to', 'contrary to', 'contrary to', 'contrary to', 'contrary to', 'with', 'with', 'with', 'along with', 'dependant on', 'dependant on', 'controle', 'without', 'without', 'without', 'without', 'without', 'without', 'without', 'in place of'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_dict_F.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83790"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(HE_dict.values())) + len(set(H_dict_F.values())) + len(set(EH_dict.keys())) + len(set(EH_dict_F.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    path = str(fname)\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11999\n",
      "11999\n",
      "First stage par dus jootey khaye Grover  se\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "           Hinglish        Hindi\n",
      "0         hajagiree       हजगिरी\n",
      "1          chekaanv        चेकॉव\n",
      "2        spinagaarn   स्पिनगार्न\n",
      "3             medal         मेडल\n",
      "4       chetthinaad    चेत्तिनाद\n",
      "...             ...          ...\n",
      "14914          roda         रोडा\n",
      "14915  shymaleshwor  स्यामलेश्वर\n",
      "14916           bar          वार\n",
      "14917       leonard    लियोनार्ड\n",
      "14918      gurudwar   गुरूद्वारा\n",
      "\n",
      "[14919 rows x 2 columns]\n",
      "After NP array\n",
      "[['hajagiree' 'हजगिरी']\n",
      " ['chekaanv' 'चेकॉव']\n",
      " ['spinagaarn' 'स्पिनगार्न']\n",
      " ...\n",
      " ['bar' 'वार']\n",
      " ['leonard' 'लियोनार्ड']\n",
      " ['gurudwar' 'गुरूद्वारा']]\n",
      "Profanity\n",
      "       Hinglish         English\n",
      "0         badir           idiot\n",
      "1    badirchand           idiot\n",
      "2       bakland           idiot\n",
      "3        bhadva            pimp\n",
      "4     bhootnika  son of a witch\n",
      "..          ...             ...\n",
      "204    vahiyaat      disgusting\n",
      "205      jihadi       terrorist\n",
      "206   atankvadi       terrorist\n",
      "207   atankwadi       terrorist\n",
      "208     aatanki        terorist\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "After NP array\n",
      "[['badir' 'idiot']\n",
      " ['badirchand' 'idiot']\n",
      " ['bakland' 'idiot']\n",
      " ['bhadva' 'pimp']\n",
      " ['bhootnika' 'son of a witch']\n",
      " ['chinaal' 'whore']\n",
      " ['chup' 'shut up']\n",
      " ['chutia' 'fucker']\n",
      " ['ghasti' 'hooker']\n",
      " ['chutiya' 'fucker']\n",
      " ['haraami' 'bastard']\n",
      " ['haraam' 'bastard']\n",
      " ['hijra' 'transsexual']\n",
      " ['hinjda' 'transsexual']\n",
      " ['jaanvar' 'animal']\n",
      " ['kutta' 'dog']\n",
      " ['kutiya' 'bitch']\n",
      " ['khota' 'donkey']\n",
      " ['auladheen' 'sonless']\n",
      " ['jaat' 'breed']\n",
      " ['najayaz' 'illegitimate']\n",
      " ['gandpaidaish' 'badborn']\n",
      " ['saala' 'sister’s husband']\n",
      " ['kutti' 'bitch']\n",
      " ['soover' 'swine']\n",
      " ['tatti' 'shit']\n",
      " ['potty' 'shit']\n",
      " ['bahenchod' 'sister fucker']\n",
      " ['bahanchod' 'sister fucker']\n",
      " ['bahencho' 'sister fucker']\n",
      " ['bancho' 'sister fucker']\n",
      " ['bahenke' 'sister’s']\n",
      " ['laude' 'dick']\n",
      " ['takke' 'balls']\n",
      " ['betichod' 'daughter fucker']\n",
      " ['bhaichod' 'brother fucker']\n",
      " ['bhains' 'buffalo']\n",
      " ['jhalla' 'faggot']\n",
      " ['jhant' 'pubic']\n",
      " ['nabaal' 'hairless']\n",
      " ['pissu' 'bug']\n",
      " ['kutte' 'dog']\n",
      " ['maadherchod' 'mother fucker']\n",
      " ['madarchod' 'motherfucker']\n",
      " ['padma' 'fat bitch']\n",
      " ['raand' 'whore']\n",
      " ['jamai' 'son-in-law']\n",
      " ['randwa' 'male prostitute']\n",
      " ['randi' 'hooker']\n",
      " ['bachachod' 'son fucker']\n",
      " ['bachichod' 'daughter fucker']\n",
      " ['soower' 'swine']\n",
      " ['bachchechod' 'children fucker']\n",
      " ['ullu' 'idiot']\n",
      " ['pathe' 'idiot']\n",
      " ['banda' 'semi-dick']\n",
      " ['booblay' 'boobs']\n",
      " ['booby' 'boobs']\n",
      " ['buble' 'boobs']\n",
      " ['babla' 'boobs']\n",
      " ['bhonsriwala' 'fucker']\n",
      " ['bhonsdiwala' 'fucker']\n",
      " ['ched' 'pussy']\n",
      " ['chut' 'pussy']\n",
      " ['chod' 'fuck']\n",
      " ['chodu' 'fucker']\n",
      " ['chodra' 'fucker']\n",
      " ['choochi' 'boobs']\n",
      " ['chuchi' 'boobs']\n",
      " ['gaandu' 'asshole']\n",
      " ['gandu' 'asshole']\n",
      " ['gaand' 'ass']\n",
      " ['lavda' 'dick']\n",
      " ['lawda' 'dick']\n",
      " ['lauda' 'dick']\n",
      " ['lund' 'dick']\n",
      " ['balchod' 'hair fucker']\n",
      " ['lavander' 'dick head']\n",
      " ['muth' 'masturbate']\n",
      " ['maacho' 'mother fucker']\n",
      " ['mammey' 'boobs']\n",
      " ['tatte' 'boobs']\n",
      " ['toto' 'penis']\n",
      " ['toota' 'broken']\n",
      " ['backar' 'gossip']\n",
      " ['bhandwe' 'pimp']\n",
      " ['bhosadchod' 'ass fucker']\n",
      " ['bhosad' 'pussy']\n",
      " ['bumchod' 'ass fucker']\n",
      " ['bum' 'ass']\n",
      " ['bur' 'pussy']\n",
      " ['chatani' 'ketchup']\n",
      " ['cunt' 'pussy']\n",
      " ['cuntmama' 'pussy']\n",
      " ['chipkali' 'lizzard']\n",
      " ['pasine' 'sweat']\n",
      " ['jhaat' 'cunt']\n",
      " ['chodela' 'fucked up']\n",
      " ['bhagatchod' 'saint fucker']\n",
      " ['chhola' 'clit']\n",
      " ['chudai' 'fucking']\n",
      " ['chudaikhana' 'whore house']\n",
      " ['chunni' 'clit']\n",
      " ['choot' 'pussy']\n",
      " ['bhoot' 'ghost']\n",
      " ['dhakkan' 'idiot']\n",
      " ['bhajiye' 'snack']\n",
      " ['fateychu' 'torn pussy']\n",
      " ['gandnatije' 'Bad result']\n",
      " ['lundtopi' 'condom']\n",
      " ['gaandu' 'ass']\n",
      " ['gaandfat' 'ass']\n",
      " ['gaandmasti' 'ass']\n",
      " ['makhanchudai' 'fucking']\n",
      " ['gaandmarau' 'ass fuck']\n",
      " ['gandu' 'faggot']\n",
      " ['chaatu' 'licker']\n",
      " ['beej' 'semen']\n",
      " ['choosu' 'sucker']\n",
      " ['fakeerchod' 'saint fucker']\n",
      " ['lundoos' 'dick']\n",
      " ['shorba' 'semen']\n",
      " ['binbheja' 'brainless']\n",
      " ['bhadwe' 'pimp']\n",
      " ['parichod' 'angel fucker']\n",
      " ['nirodh' 'condom.']\n",
      " ['pucchi' 'pussy']\n",
      " ['baajer' 'fucker']\n",
      " ['choud' 'fuck']\n",
      " ['bhosda' 'pussy']\n",
      " ['sadi' 'stinking']\n",
      " ['choos' 'suck']\n",
      " ['maka' 'mother’s']\n",
      " ['chinaal' 'prostitute']\n",
      " ['gadde' 'boobs']\n",
      " ['joon' 'bug']\n",
      " ['chullugand' 'handful dirt']\n",
      " ['doob' 'drown']\n",
      " ['khatmal' 'bug']\n",
      " ['gandkate' 'ass']\n",
      " ['bambu' 'bamboo']\n",
      " ['lassan' 'garlic']\n",
      " ['danda' 'stick']\n",
      " ['keera' 'bug']\n",
      " ['keeda' 'bug']\n",
      " ['hazaarchu' 'thousand pussy']\n",
      " ['paidaishikeeda' 'born bug']\n",
      " ['kali' 'nigger']\n",
      " ['safaid' 'american']\n",
      " ['poot' 'son']\n",
      " ['behendi' 'sister']\n",
      " ['chus' 'sucker']\n",
      " ['machudi' 'mother fucker']\n",
      " ['chodoonga' 'fuck']\n",
      " ['baapchu' 'father pussy']\n",
      " ['laltern' 'lantern']\n",
      " ['suhaagchudai' 'wedding fuck']\n",
      " ['raatchuda' 'night fuck']\n",
      " ['kaalu' 'migga']\n",
      " ['neech' 'low caste']\n",
      " ['chikna' 'gay']\n",
      " ['meetha' 'gay']\n",
      " ['beechka' 'gay']\n",
      " ['chooche' 'boobs']\n",
      " ['patichod' 'husband']\n",
      " ['rundi' 'prostitute']\n",
      " ['makkhi' 'fly']\n",
      " ['biwichod' 'wife fucker']\n",
      " ['chodhunga' 'fuck']\n",
      " ['haathi' 'elephant']\n",
      " ['kute' 'dog']\n",
      " ['jhanten' 'pubic hair']\n",
      " ['kaat' 'cut']\n",
      " ['gandi' 'filthy']\n",
      " ['gadha' 'donkey']\n",
      " ['bimaar' 'ill']\n",
      " ['badboodar' 'smelly']\n",
      " ['dum' 'tail']\n",
      " ['raandsaala' 'sister’s brother pimp']\n",
      " ['phudi' 'pussy']\n",
      " ['chute' 'pussy']\n",
      " ['kussi' 'ass']\n",
      " ['khandanchod' 'family fucker']\n",
      " ['ghussa' 'fuck']\n",
      " ['maarey' 'dead']\n",
      " ['chipkili' 'lizard']\n",
      " ['unday' 'eggs']\n",
      " ['budh' 'cunt']\n",
      " ['chaarpai' 'cot']\n",
      " ['chodun' 'fuck']\n",
      " ['chatri' 'condom']\n",
      " ['chode' 'fuck']\n",
      " ['chodho' 'fuck']\n",
      " ['mullekatue' 'Derogatory abuse to muslims']\n",
      " ['mullikatui' 'Derogatory Abuse to female muslim']\n",
      " ['mullekebaal' 'Derogatory Abuse to muslim']\n",
      " ['momedankatue' 'Derogatory Abuse to muslim']\n",
      " ['katua' 'dick cut']\n",
      " ['chutiyapa' 'fuck all']\n",
      " ['bc' 'sister fucker']\n",
      " ['mc' 'mother fucker']\n",
      " ['chudwaya' 'fuck']\n",
      " ['kutton' 'dog']\n",
      " ['jungli' 'wild']\n",
      " ['vahiyaat' 'disgusting']\n",
      " ['jihadi' 'terrorist']\n",
      " ['atankvadi' 'terrorist']\n",
      " ['atankwadi' 'terrorist']\n",
      " ['aatanki' 'terorist']]\n",
      "['randtv', 'tumhare', 'najayaz', 'baap', 'is', 'area', 'hai', 'ki', 'waha', 'koi', 'nahi', 'has', 'sakta', 'haraami', 'azad', 'mulk', 'hai', 'sab', 'jagah', 'jayenge']\n",
      "100\n",
      "najayaz\n",
      "1-illegitimate\n",
      "100\n",
      "haraami\n",
      "1-bastard\n",
      "80\n",
      "hain\n",
      "1-buffalo\n",
      "75\n",
      "adat\n",
      "1-breed\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "80\n",
      "hotay\n",
      "1-donkey\n",
      "77\n",
      "bharosa\n",
      "1-pussy\n",
      "75\n",
      "the\n",
      "1-idiot\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "100\n",
      "mc\n",
      "1-mother fucker\n",
      "100\n",
      "bc\n",
      "1-sister fucker\n",
      "91\n",
      "ghuss\n",
      "1-fuck\n",
      "75\n",
      "maar\n",
      "1-mother’s\n",
      "75\n",
      "kaat\n",
      "1-breed\n",
      "80\n",
      "sawal\n",
      "1-sister’s husband\n",
      "75\n",
      "iss\n",
      "1-bug\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "maan\n",
      "1-mother’s\n",
      "100\n",
      "chup\n",
      "1-shut up\n",
      "75\n",
      "katy\n",
      "1-cut\n",
      "75\n",
      "jaty\n",
      "1-breed\n",
      "80\n",
      "khtay\n",
      "1-donkey\n",
      "75\n",
      "ghus\n",
      "1-sucker\n",
      "75\n",
      "haramilog\n",
      "1-bastard\n",
      "80\n",
      "payda\n",
      "1-fat bitch\n",
      "77\n",
      "chinta\n",
      "1-whore\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "80\n",
      "gande\n",
      "1-asshole\n",
      "75\n",
      "kha\n",
      "1-donkey\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "gandi\n",
      "1-hooker\n",
      "75\n",
      "krte\n",
      "1-dog\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "udana\n",
      "1-stick\n",
      "80\n",
      "kutto\n",
      "1-dog\n",
      "80\n",
      "baar\n",
      "1-gossip\n",
      "75\n",
      "bnd\n",
      "1-semi-dick\n",
      "80\n",
      "br\n",
      "1-pussy\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "haat\n",
      "1-breed\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "89\n",
      "kute\n",
      "1-dog\n",
      "83\n",
      "chopra\n",
      "1-fucker\n",
      "80\n",
      "khola\n",
      "1-donkey\n",
      "80\n",
      "gadhe\n",
      "1-boobs\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "padai\n",
      "1-fat bitch\n",
      "89\n",
      "bhot\n",
      "1-ghost\n",
      "75\n",
      "sahi\n",
      "1-stinking\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "75\n",
      "sali\n",
      "1-stinking\n",
      "92\n",
      "chutiya\n",
      "1-fucker\n",
      "75\n",
      "kaam\n",
      "1-cut\n",
      "80\n",
      "bhai\n",
      "1-buffalo\n",
      "80\n",
      "chutiyapa\n",
      "1-fucker\n",
      "91\n",
      "kuttey\n",
      "1-dog\n",
      "100\n",
      "tatti\n",
      "1-shit\n",
      "75\n",
      "gali\n",
      "1-nigger\n",
      "75\n",
      "dna\n",
      "1-stick\n",
      "75\n",
      "aata\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "wali\n",
      "1-nigger\n",
      "77\n",
      "chudian\n",
      "1-fucker\n",
      "89\n",
      "shadi\n",
      "1-stinking\n",
      "75\n",
      "baat\n",
      "1-breed\n",
      "75\n",
      "utha\n",
      "1-masturbate\n",
      "75\n",
      "mai\n",
      "1-son-in-law\n",
      "89\n",
      "bana\n",
      "1-semi-dick\n",
      "80\n",
      "bandh\n",
      "1-semi-dick\n",
      "80\n",
      "padha\n",
      "1-fat bitch\n",
      "100\n",
      "katua\n",
      "1-dick cut\n",
      "80\n",
      "kutte\n",
      "1-dog\n",
      "['रॉड', 'तुम्हारे', 'illegitimate', 'बाप', 'है', 'area', 'है', 'की', 'स्वाहा', 'की', 'नहीं', 'हैट्स', 'सांता', 'bastard', 'आज़ाद', 'माल्क', 'है', 'सब', 'जयगढ़', 'जाएगा']\n",
      "['first', 'stage', 'par', 'डेटस्कलैंड', 'टूहे', 'श्रेय', 'ओवर', 'से']\n",
      "['guys', 'counter', 'modi', 'govt', 'decisions', 'fact', 'black', 'money', 'cleanup', 'stand', 'taken', 'many', 'discussions', 'news', 'channel', 'individuals', 'meetings', 'much', 'efforts', 'made', 'media', 'make', 'people', 'scared', 'provoke']\n"
     ]
    }
   ],
   "source": [
    "processed_tokens = get_processed_tokens_for_english_tweets()\n",
    "processed_Hindi_tokens = get_processed_hindi_tokens()\n",
    "t_dict = get_transliteration_Hinglish_Hindi_dict()\n",
    "P_dict = get_profanity_dict()\n",
    "HE_dict, H_dict_F, EH_dict, EH_dict_F = translate_hindi_to_english()\n",
    "processed_Hindi_tokens = get_translated_hindi_english(HE_dict, H_dict_F)\n",
    "processed_Hindi_tokens = get_token_translations(processed_tokens, processed_Hindi_tokens, EH_dict, P_dict, t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['रॉड',\n",
       " 'तुम्हारे',\n",
       " 'illegitimate',\n",
       " 'बाप',\n",
       " 'है',\n",
       " 'area',\n",
       " 'है',\n",
       " 'की',\n",
       " 'स्वाहा',\n",
       " 'की',\n",
       " 'नहीं',\n",
       " 'हैट्स',\n",
       " 'सांता',\n",
       " 'bastard',\n",
       " 'आज़ाद',\n",
       " 'माल्क',\n",
       " 'है',\n",
       " 'सब',\n",
       " 'जयगढ़',\n",
       " 'जाएगा']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_Hindi_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,hidden_dim,kernel_size,feature_linear, word_vector_list):\n",
    "        super(MIMCT, self).__init__()\n",
    "        \n",
    "        #Primary Input\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight.data.copy_(torch.from_numpy(word_vector_list))\n",
    "        \n",
    "        #Secondary Input (Profanity Vector, LiWC and Sentiment Score)\n",
    "        \n",
    "        self.CNN_Layers = nn.Sequential( \n",
    "            nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),\n",
    "            nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),\n",
    "            nn.Flatten(),nn.Dropout(p=0.25),\n",
    "            nn.Linear(feature_linear, 3),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "        \n",
    "        \n",
    "        #create LSTM.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim,3)\n",
    "        self.dropout = nn.Dropout(p=0.20)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        self.linear = nn.Linear(input_channel+1,3)\n",
    "    def forward(self,x):\n",
    "        cnn_output = self.CNN_Layers(x)\n",
    "      #  y = self.LSTM_Layers(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out= self.dropout(lstm_out)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(lstm_out.size(1), -1))\n",
    "        lstm_output = F.log_softmax(tag_space, dim=1)\n",
    "        #concat the outputs the compile layer with categorical cross-entropy the loss function,\n",
    "        lstm_output = lstm_output.view(lstm_output.size(0),-1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0),-1)\n",
    "        \n",
    "        X = torch.cat((lstm_output,cnn_output))\n",
    "        X = X.view(1,X.size(0),X.size(1))\n",
    "        X = self.maxpool(X)\n",
    "        \n",
    "        X = self.linear(X.view(X.size(2), -1))\n",
    "        X = self.softmax(X)\n",
    "        print(X)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting English Dataset\n",
    "data_vector_English = load_vectors(\"cc.en.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_list = []\n",
    "for i, word in enumerate(data_vector_English):\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    word_vector_list.append(list(data_vector[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2235, 0.4550, 0.3215]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-cfeef8848fde>:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = self.softmax(X)\n"
     ]
    }
   ],
   "source": [
    "#Parameters for LSTM\n",
    "hidden_dim = 64\n",
    "dropout = 0.25, \n",
    "#recurrent_dropout = 0.3\n",
    "\n",
    "#The length vocabulary formed after translating the Hindi and Hinglish tokens to English along with English Tokens \n",
    "vocab_size = len(set(HE_dict.values())) + len(set(H_dict_F.values())) + len(set(EH_dict.keys())) + len(set(EH_dict_F.keys()))\n",
    "\n",
    "model = MIMCT(input_channel, output_channel, embedding_dim, hidden_dim, kernel_size, feature_linear)\n",
    "\n",
    "#Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#output = model(input1)\n",
    "#create the loss criterion and training loops\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "print(input1.shape)\n",
    "output = model(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "m = nn.Conv1d(1, 2,1,stride=2)\n",
    "input1 = torch.randn(10)\n",
    "cnn1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n",
    "input1 = input1.unsqueeze(0).unsqueeze(0)\n",
    "input1.shape\n",
    "cnn1d_2(input1)\n",
    "\n",
    "\n",
    "\n",
    "#------------CNN_Class----------------#\n",
    "kernel_size = [4,3,2]\n",
    "embedding_dim = 10\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "\n",
    "Layer1 = nn.Conv1d(16, 16, 4, stride=1)\n",
    "Layer2 = nn.Conv1d(16, 16, 3, stride=1)\n",
    "Layer3 = nn.Conv1d(16, 16, 2, stride=1)\n",
    "Dropout_layer = nn.Dropout(p=0.25)\n",
    "dense_layer = nn.Linear(Feature_layer3, 3)\n",
    "nn.Flatten\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 10\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "output = Layer1(input1)\n",
    "output1 = Layer2(output)\n",
    "output2 = Layer3(output1)\n",
    "output3 = Dropout_layer(output2)\n",
    "output4 = dense_layer(output3)\n",
    "nn.flatten\n",
    "\n",
    "\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],[0.0,  1.0],[0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)\n",
    "m.weight\n",
    "\n",
    "class MIMCT(nn.Module):   \n",
    "    def __init__(self,input_channel,output_channel,embedding_dim,kernel_size,feature_linear):\n",
    "        super(MIMCT, self).__init__()\n",
    "        self.CNN_Layers = nn.Sequential( nn.Conv1d(input_channel, output_channel,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.CNN_Layers(x)\n",
    "        return x\n",
    "\n",
    "#try with output channel 1 as well.\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "output_channel = 16\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "model = MIMCT(input_channel,output_channel,embedding_dim,kernel_size,feature_linear)\n",
    "\n",
    "output = model(input1)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''''\n",
    "kernel_size = [20,15,10]\n",
    "embedding_dim = 200\n",
    "Feature_layer1 = embedding_dim - kernel_size[0] + 1\n",
    "Feature_layer2 = Feature_layer1 - kernel_size[1] + 1\n",
    "Feature_layer3 = Feature_layer2 - kernel_size[2] + 1\n",
    "feature_linear = Feature_layer3 * input_channel\n",
    "m = nn.Sequential( nn.Conv1d(input_channel,16,kernel_size[0], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[1], stride=1),nn.Conv1d(input_channel, output_channel, kernel_size[2], stride=1),nn.Flatten(),nn.Dropout(p=0.25),nn.Linear(feature_linear, 3),nn.Softmax())\n",
    "\n",
    "#------------Forward------------------#\n",
    "batch_size = 1\n",
    "input_channel = 16\n",
    "embedding_dim = 200\n",
    "input1 = torch.randn(batch_size,input_channel,embedding_dim)\n",
    "test_input = input1\n",
    "output = m(test_input)\n",
    "nn.flatten\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(16, 3)\n",
    "b = torch.randn(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
